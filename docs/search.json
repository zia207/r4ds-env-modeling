[
  {
    "objectID": "data-export-import.html",
    "href": "data-export-import.html",
    "title": "Data Science with R For Envrionmental Modeling",
    "section": "",
    "text": "Reading data for analysis and exporting the results to another system for report writing can be done efficiently with R. There are multiple ways to import and export data to/from R. In this tutorial, you will learn some most common ways to read and write data with R.\nAll data set use in this exercise can be downloaded from my dropbox folder.\n\n\nIt would be best if you created a working directory in R to read and write files locally. The following example shows how to create the working directory in R.\nBefore creating a working directory, you may check the directory of your current R session; the function getwd() will print the current working directory path as a string.\n\n\nCode\ngetwd()\n\n\n[1] \"F:/GitHub/R_QuartoWebsite/r4ds-env-modeling\"\n\n\nIf you want to change the working directory in R you just need to call the setwd() function, specifying as argument the path of the new working directory folder.\n\nsetwd(“F:\\R-Project”) setwd(“F:/R-Project”\n\nRemember that you must use the forward slash / or double backslash \\ in R! The Windows format of single backslash will not work.\nThe files under in a directory can check using dir() function\n\ndir()\n\n\n\n\n\n\nImporting data into R becomes time intensive. The easiest way to import data into R is by using RStudio IDE. This feature can be accessed from the Environment pane or from the **tools menu*. The importers are grouped into three categories: Text data, Excel data, and statistical data. The detail could be found here.\nTo access this feature, use the “Import Dataset” dropdown from the “Environment” pane:\n\n\nFigure 1: Data Import RStudio Environment Pane\n\n\n\nOr through the “File” menu, followed by the “Import Dataset” submenu:\n\n\nFigure 2: Data Import RStudio File Menu\n\n\n\n\n\n\nThe easiest form of data to import into R is a simple text file. The primary function to import from a text file is read.table().\n\nread.table(file, header = FALSE, sep = ““, quote =”“’”,…..)\n\n\n\nCode\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read .txt file\ndf.txt<-read.table(paste0(dataFolder,\"test_data.txt\"), header= TRUE) \n#data.txt<-read.table(\"test_data.txt\", header= TRUE) \nhead(df.txt)\n\n\n   ID treat  var rep    PH   TN   PN   GW ster   DTM   SW   GAs  STAs\n1 Low    As BR01   1  84.0 28.3 27.7 35.7 20.5 126.0 28.4 0.762 14.60\n2 Low    As BR01   2 111.7 34.0 30.0 58.1 14.8 119.0 36.7 0.722 10.77\n3 Low    As BR01   3 102.3 27.7 24.0 44.6  5.8 119.7 32.9 0.858 12.69\n4 Low    As BR06   1 118.0 23.3 19.7 46.4 20.3 119.0 40.0 1.053 18.23\n5 Low    As BR06   2 115.3 16.7 12.3 19.9 32.3 120.0 28.2 1.130 13.72\n6 Low    As BR06   3 111.0 19.0 15.3 35.9 14.9 116.3 42.3 1.011 15.97\n\n\nCode\nnames(df.txt)\n\n\n [1] \"ID\"    \"treat\" \"var\"   \"rep\"   \"PH\"    \"TN\"    \"PN\"    \"GW\"    \"ster\" \n[10] \"DTM\"   \"SW\"    \"GAs\"   \"STAs\" \n\n\nHowever, scan() function could be used to scan and read data. It is usually used to read data into vector or list or from file in R Language.\n\nsacn(scan(file = ““, what = double(), nmax = -1, n = -1, sep =”“,..)\n\n\n\nCode\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# Scan data \ndf.scan<-scan(paste0(dataFolder,\"test_data.txt\"),  what = list(\"\", \"\", \"\"))   \n\n\n\n\n\nA comma delimited or comma-separated file (CSV) is one where each value in the file is separated by a comma, although other characters can be used. Reading data from a CSV file is made easy by the read.csv(), an extension of read.table(). It facilitates the direct import of data from CSV files.\n\nread.csv(file, header = TRUE, sep = “,”, quote = “““,…)\n\n\n\nCode\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read .csv file\ndf.csv<-read.csv(paste0(dataFolder,\"test_data.csv\"), header= TRUE) \n#head(data.csv)\n\n\n\n\n\nOne of the best ways to read an Excel file is to export it to a comma-delimited file and import it using the method above. Alternatively, we can use the xlsx package to access Excel files. The first row should contain variable/column names.\n\ninstall.packages(“xlsx”)\n\n\n\nCode\n# Load xlsx package\nlibrary(xlsx)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# Import Sheet 1, from a excel file\ndata.xlsx <-xlsx::read.xlsx(paste0(dataFolder,\"test_data.xlsx\"), 1) \n#data.xlsx <-read.xlsx(\"test_data.xlsx\", 1)  \nnames(data.xlsx)\n\n\n [1] \"ID\"    \"treat\" \"var\"   \"rep\"   \"PH\"    \"TN\"    \"PN\"    \"GW\"    \"ster\" \n[10] \"DTM\"   \"SW\"    \"GAs\"   \"STAs\" \n\n\n\n\n\nJSON is an open standard file and lightweight data-interchange format that stands for JavaScript Object Notation. The JSON file is a text file that is language independent, self-describing, and easy to understand.\nThe JSON file is read by R as a list using the function fromJSON() of rjson package.\n\ninstall.packages(“rjson”)\n\n\nfromJSON(json_str, file, method = “C”, unexpected.escape = “error”, sim..)\n\n\n\nCode\n# Load rjson package\nlibrary(rjson)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read .json file\ndf.json <- rjson::fromJSON(file= paste0(dataFolder, \"test_data.json\"),  simplify=TRUE)\n#print(df.json)\n\n\nWe can convert to data frame\n\n\nCode\ndf.json <- as.data.frame(df.json)\nhead(df.json)\n\n\n  ID  treat  var rep    PH   TN   PN   GW ster   DTM   SW   GAs  STAs\n1  1 Low As BR01   1  84.0 28.3 27.7 35.7 20.5 126.0 28.4 0.762 14.60\n2  2 Low As BR01   2 111.7 34.0 30.0 58.1 14.8 119.0 36.7 0.722 10.77\n3  3 Low As BR01   3 102.3 27.7 24.0 44.6  5.8 119.7 32.9 0.858 12.69\n4  4 Low As BR06   1 118.0 23.3 19.7 46.4 20.3 119.0 40.0 1.053 18.23\n5  5 Low As BR06   2 115.3 16.7 12.3 19.9 32.3 120.0 28.2 1.130 13.72\n6  6 Low As BR06   3 111.0 19.0 15.3 35.9 14.9 116.3 42.3 1.011 15.97\n\n\n\n\n\n\nforeign packages is mostly used to read data stored by Minitab, S, SAS, SPSS, Stata, Systat, dBase, and so forth.\n\ninstall.packages(“foreign”)\n\nHaven enables R to read and write various data formats used by other statistical packages by wrapping with ReadStat C library. written b Haven is part of the tidyverse. Current it support SAS, SPSS and Stata files\nread.dta() function from foreign package can reads a file in Stata version 5-12 binary format (.dta) into a data frame.\n\n\n\nForeign - read.dta()Haven read_dta()\n\n\n\n\nCode\n# Load foreign package\nlibrary(foreign)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read .dta file\ndf.dta_01 <- foreign::read.dta(paste0(dataFolder,\"test_data.dta\")) \n\n\n\n\n\n\nCode\n# Load foreign package\nlibrary(haven)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read .dta file\ndf.dta_02 <- haven::read_dta(paste0(dataFolder,\"test_data.dta\")) \n\n\n\n\n\n\n\n\n\nForeign - read.spss()Haven read_sav()\n\n\n\n\nCode\n# Load foreign package\nlibrary(foreign)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read .sav file\ndf.sav_01 <- foreign::read.spss(paste0(dataFolder,\"test_data.sav\")) \n\n\n\n\n\n\nCode\n# Load haven package\nlibrary(haven)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read .sav file\ndf.sav_02 <- haven::read_sav(paste0(dataFolder,\"test_data.sav\")) \n#head(df.sav)\n\n\n\n\n\n\n\n\nread_sas() function from haven package can read sas (.sas7bdat) file easily.\n\n\nCode\n# Load haven package\nlibrary(haven)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read .sas7bdat file\ndf.sas <- haven::read_sas(paste0(dataFolder,\"test_data.sas7bdat\")) \n#head(df.sas)\n\n\n\n\n\n\nTo export data and save to your local drive, you need the file path and an extension. First of all, the path is the location where the data will be stored.\nBefore we start, we need to specify the working directory in which we can export the data.\n\n\nFirst of all, let create a data frame that we will going to export as a text/CSV file.\n\n\nCode\nVariety =c(\"BR1\",\"BR3\", \"BR16\", \"BR17\", \"BR18\", \"BR19\",\"BR26\",\n          \"BR27\",\"BR28\",\"BR29\",\"BR35\",\"BR36\") # create a text vector\nYield = c(5.2,6.0,6.6,5.6,4.7,5.2,5.7,\n                5.9,5.3,6.8,6.2,5.8) # create numerical vector\nrice.data= data.frame(Variety, Yield)\n\n\nThe popular R base functions for writing data are write.table(), write.csv(), write.csv2() and write.delim() functions.\nBefore start, you need to specify the working or destination directory in where you will save the data.\n\n\nCode\n# Define destination folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\nwrite.csv(rice.data, paste0(dataFolder, \"rice_data.csv\"), row.names = F) # no row names\n# write.csv(rice.data, \"rice_data.csv\", row.names = F) # no row names\n\n\n\n\n\nExporting data from R to Excel can be achieved with several packages. The most known package to export data frames or tables as Excel is xlsx, that provides the write.xlsx and write.xlsx2 functions.\n\n\nCode\n# load xlsx\nlibrary(xlsx)\n# Define destination folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# write as xlsx file\nxlsx::write.xlsx(rice.data, paste0(dataFolder, \"rice_data.xlsx\"))\n\n\n\n\n\nTo write JSON Object to file, the toJSON() function from the rjson library can be used to prepare a JSON object and then use the write() function for writing the JSON object to a local file.\n\n\nCode\n# Load rjson package\nlibrary(rjson)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# create a JSON object\njsonData <-rjson::toJSON(rice.data)\n# write JSON objects\nwrite(jsonData, file= paste0(dataFolder,\"rice_data.json\"))\n\n\n\n\n\nIf you want to share the data from R as Objects and share those with your colleagues through different systems so that they can use it right away into their R-workspace. These objects are of two types .rda/.RData which can be used to store some or all objects, functions from R global environment.\nThe save() function allows us to save multiple objects into our global environment:\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you specify save.image(file = “R_objects.RData”) Export all objects (the workspace image).\n\n\nTo save only one object it is more recommended saving it as RDS with the saveRDS() function:\n\n\nCode\n# write .RDS file\nsaveRDS(rice.data, \"E:/Dropbox/GitHub/Data/rice_data.rds\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you specify compress = TRUE as argument of the above functions the file will be compressed by default as gzip.\n\n\n\n\n\n\n\n\nIf you want export data from R to STATA, you will need to use the write.dta() function of the foreign package. This package provides functions for r\n\n\nCode\n# Load foreign package\nlibrary(foreign)\n# define data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# write dta file\nforeign::write.dta(rice.data, file= paste0(dataFolder,\"rice_data.dta\"))\n\n\n\n\n\nHaven enables R to read and write various data formats used by other statistical packages by wrapping with ReadStat C library. written b Haven is part of the tidyverse. Current it support SAS, SPSS and Stata files\nThe write_sav() function of haven package can be used to export R-object to SPSS\n\n\nCode\n# Load haven package\nlibrary(haven)\n# write .sav file\nhaven::write_sav(rice.data, \"E:/Dropbox/GitHub/Data/rice_data.sav\")\n\n\n\n\n\nThe write_sas() function of haven package can be used to export R-object to SAS (.sas7bdat)\n\n\nCode\n# Load haven package\nlibrary(haven)\n# write .sav file\nhaven::write_sas(rice.data, \"E:/Dropbox/GitHub/Data/rice_data.sas7bdat\")\n\n\n\n\n\n\nThe R-package data.tabel a improved version of data.frame, the a R-base function. It has following advantages:\n• concise syntax: fast to type, fast to read • fast speed • memory efficient\n\ninstall.packages(“data.table”)\n\nWe will use two important functions of data.table to read and write CSV files in R.\n\n\nfread: This function is capable of reading CSV files quickly and conveniently.\n\nfread(input, file,….)\n\n\n\nCode\n# Load library\nlibrary(data.table)\n# define data location\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read with fread()\nDT<-data.table::fread(paste0(dataFolder,\"LBC_data.csv\"), header= TRUE) \nstr(DT)\n\n\nClasses 'data.table' and 'data.frame':  3110 obs. of  25 variables:\n $ REGION_ID         : int  3 3 3 3 3 3 3 3 3 3 ...\n $ STATE             : chr  \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ County            : chr  \"Baldwin County\" \"Butler County\" \"Butler County\" \"Chambers County\" ...\n $ Empty Column 1    : logi  NA NA NA NA NA NA ...\n $ X                 : num  789778 877732 877732 984215 726606 ...\n $ Y                 : num  884557 1007286 1007286 1148649 1023616 ...\n $ Fips              : int  1003 1013 1013 1017 1023 1025 1031 1035 1039 1041 ...\n $ LCB Mortality Rate: num  48.1 38.3 38.3 49.6 31.8 42 53.7 46.9 65.5 57.1 ...\n $ Smoking           : num  20.8 26 26 25.1 21.8 22.6 21.2 24.9 25.9 22.9 ...\n $ PM  25            : num  7.89 8.46 8.46 8.87 8.58 8.42 8.42 8.23 8.24 8.45 ...\n $ NO2               : num  0.794 0.634 0.634 0.844 0.593 ...\n $ SO2               : num  0.0353 0.0135 0.0135 0.0482 0.024 ...\n $ Ozone             : num  39.8 38.3 38.3 40.1 37.1 ...\n $ Pop 65            : num  19.5 19 19 18.9 22.1 19 16.3 21.6 20.5 18.3 ...\n $ Pop Black         : num  9.24 43.94 43.94 39.24 41.94 ...\n $ Pop Hipanic       : num  4.54 1.26 1.26 2.14 0.86 1.34 6.76 1.84 1.62 1.86 ...\n $ Pop White         : num  83.1 52.6 52.6 56.4 56.3 ...\n $ Education         : int  66 38 38 47 55 39 60 35 53 44 ...\n $ Poverty %         : num  13.1 26.1 26.1 21.5 23.1 ...\n $ Income Equality   : num  4.5 5.1 5.1 4.7 5.8 8.2 4.8 4.9 4.6 5.8 ...\n $ Uninsured         : num  13.3 12.7 12.7 13.3 12.9 ...\n $ DEM               : num  36.8 111.7 111.7 227 68.2 ...\n $ Radon Zone Class  : chr  \"Zone-3\" \"Zone-3\" \"Zone-3\" \"Zone-3\" ...\n $ Urban Rural       : chr  \"Medium/small metro\" \"Nonmetro\" \"Nonmetro\" \"Nonmetro\" ...\n $ Coal Production   : chr  \"No\" \"No\" \"No\" \"No\" ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\n\nCode\n#DT<-fread(\"usa_geochemical.csv\", header= TRUE) \n\n\nYou cam compare data reading time of fread() and read.csv() functions:\n\nR-base function read.csvdata.table fread\n\n\n\n\nCode\nsystem.time(read.csv(paste0(dataFolder,\"LBC_data.csv\"), header= TRUE)) \n\n\n   user  system elapsed \n   0.05    0.00    0.04 \n\n\n\n\n\n\nCode\nsystem.time(data.table::fread(paste0(dataFolder,\"LBC_data.csv\"), header= TRUE))\n\n\n   user  system elapsed \n      0       0       0 \n\n\n\n\n\n\n\n\nfwrite(): This capable to write CSV field very fast!\n\nfwrite(x, file = ““, append = FALSE, quote =”auto”, …..)\n\n\n\nCode\n# define data location\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# read with fread()\ndata.table::fwrite(DT,  paste0(dataFolder, \"DT.csv\"), row.names=F, quote=TRUE)\n\n\nNow we compare writing time of frwite functions with write.csv functions.\n\nR-base function write.csvdata.table fwrite\n\n\n\nCode\nsystem.time(write.csv(DT,  paste0(dataFolder, \"DT.csv\"), row.names=F))\n\nuser system elapsed 0.20 0.00 0.21\n\n\n\n\nCode\nsystem.time(data.table::fwrite(DT,  paste0(dataFolder, \"DT.csv\"), row.names=F, quote=TRUE))\n\n\n   user  system elapsed \n   0.01    0.00    0.01 \n\n\n\n\n\n\n\n\n\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames. It has a few specific design goals:\n\nLightweight, minimal API: make pushing data frames in and out of memory as simple as possible\nLanguage agnostic: Feather files are the same whether written by Python or R code. Other languages can read and write Feather files, too.\n\nFeather is extremely fast. Since Feather does not currently use any compression internally, it works best when used with solid-state drives as come with most of today’s laptop computers. For this first release, we prioritized a simple implementation and are thus writing unmodified Arrow memory to disk source.\n\ninstall.packages(“feather”)\n\n\n\nCode\n#install.packages(\"feather\")\nlibrary(feather)\n\n\nFirst we have to create feather data using write_feather() function\n\nwrite_feather(x, path)\n\n\n\nCode\n# define data location\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\n# Load feather\nlibrary(feather)\n# write_feather()\nfeather::write_feather(DT, paste0(dataFolder, \"LBC_data.feather\"))\n\n\nWe can read this feather data with lighting speed using read_feather function()\n\n\nCode\nDT_feather <- feather::read_feather(paste0(dataFolder, \"LBC_data.feather\"))\nstr(DT_feather)\n\n\ntibble [3,110 × 25] (S3: tbl_df/tbl/data.frame)\n $ REGION_ID         : int [1:3110] 3 3 3 3 3 3 3 3 3 3 ...\n $ STATE             : chr [1:3110] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ County            : chr [1:3110] \"Baldwin County\" \"Butler County\" \"Butler County\" \"Chambers County\" ...\n $ Empty Column 1    : logi [1:3110] NA NA NA NA NA NA ...\n $ X                 : num [1:3110] 789778 877732 877732 984215 726606 ...\n $ Y                 : num [1:3110] 884557 1007286 1007286 1148649 1023616 ...\n $ Fips              : int [1:3110] 1003 1013 1013 1017 1023 1025 1031 1035 1039 1041 ...\n $ LCB Mortality Rate: num [1:3110] 48.1 38.3 38.3 49.6 31.8 42 53.7 46.9 65.5 57.1 ...\n $ Smoking           : num [1:3110] 20.8 26 26 25.1 21.8 22.6 21.2 24.9 25.9 22.9 ...\n $ PM  25            : num [1:3110] 7.89 8.46 8.46 8.87 8.58 8.42 8.42 8.23 8.24 8.45 ...\n $ NO2               : num [1:3110] 0.794 0.634 0.634 0.844 0.593 ...\n $ SO2               : num [1:3110] 0.0353 0.0135 0.0135 0.0482 0.024 ...\n $ Ozone             : num [1:3110] 39.8 38.3 38.3 40.1 37.1 ...\n $ Pop 65            : num [1:3110] 19.5 19 19 18.9 22.1 19 16.3 21.6 20.5 18.3 ...\n $ Pop Black         : num [1:3110] 9.24 43.94 43.94 39.24 41.94 ...\n $ Pop Hipanic       : num [1:3110] 4.54 1.26 1.26 2.14 0.86 1.34 6.76 1.84 1.62 1.86 ...\n $ Pop White         : num [1:3110] 83.1 52.6 52.6 56.4 56.3 ...\n $ Education         : int [1:3110] 66 38 38 47 55 39 60 35 53 44 ...\n $ Poverty %         : num [1:3110] 13.1 26.1 26.1 21.5 23.1 ...\n $ Income Equality   : num [1:3110] 4.5 5.1 5.1 4.7 5.8 8.2 4.8 4.9 4.6 5.8 ...\n $ Uninsured         : num [1:3110] 13.3 12.7 12.7 13.3 12.9 ...\n $ DEM               : num [1:3110] 36.8 111.7 111.7 227 68.2 ...\n $ Radon Zone Class  : chr [1:3110] \"Zone-3\" \"Zone-3\" \"Zone-3\" \"Zone-3\" ...\n $ Urban Rural       : chr [1:3110] \"Medium/small metro\" \"Nonmetro\" \"Nonmetro\" \"Nonmetro\" ...\n $ Coal Production   : chr [1:3110] \"No\" \"No\" \"No\" \"No\" ...\n\n\nCompare to and write.csv() as well as frwite(), write_feather() is very fast:\n\nR-base write.csvdata.table fwriteFeather write_feather\n\n\n\n\nCode\nsystem.time(write.csv(DT, paste0(dataFolder, \"LBC_data.csv\")))\n\n\n   user  system elapsed \n   0.20    0.00    0.22 \n\n\n\n\n\n\nCode\n# CSV file with fwrite\nsystem.time(data.table::fwrite(DT, paste0(dataFolder, \"LBC_data.csv\")))\n\n\n   user  system elapsed \n   0.02    0.00    0.01 \n\n\n\n\n\n\nCode\n# feather file with \nsystem.time(feather::write_feather(DT, paste0(dataFolder, \"LBC_data.feather\")))\n\n\n   user  system elapsed \n      0       0       0 \n\n\n\n\n\n\n\n\nThe tidyverse is an collection of several R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures (Source).\nThe core tidyverse includes the packages that you’re likely to use in everyday data analyses. As of tidyverse 1.3.0, the following packages are included in the core tidyverse:\n\n\nFigure 3: Core R-Packages\n\n\n\n\n\n\nInstall all the packages in the tidyverse by running install.packages(“tidyverse”).\nRun library(tidyverse) to load the core tidyverse with and make it available in your current R session.\n\n\ninstall.packages(“tidyverse”)\n\n\n\nCode\nlibrary(tidyverse)\n\n\nAs well as readr, for reading flat files, the tidyverse package installs a number of other packages for reading data:\n\nDBI for relational databases. You’ll need to pair DBI with a database specific backends like RSQLite, RMariaDB, RPostgres, or odbc. Learn more at https://db.rstudio.com.\nhaven for SPSS, Stata, and SAS data.\nhttr for web APIs.\nreadxl for .xls and .xlsx sheets.\ngooglesheets4 for Google Sheets via the Sheets API v4.\ngoogledrive for Google Drive files.\nrvest for web scraping.\njsonlite for JSON. (Maintained by Jeroen Ooms.)\nxml2 for XML\n\n\n\n\nA tibble, or tbl_df, is the latest method for reimagining of modern data-frame and It keeps all the crucial features regarding the data frame. Since R is an old language, and some things that were useful 10 or 20 years ago now get in your way. It’s difficult to change base R without breaking existing code, so most innovation occurs in tibble() data-frame with tibble package.\nKey features of Tibble\n\nA Tibble never alters the input type.\nWith Tibble, there is no need for us to be bothered about the automatic changing of characters to strings.\nTibbles can also contain columns that are the lists.\nWe can also use non-standard variable names in Tibble.\nWe can start the name of a Tibble with a number, or we can also contain space.\nTo utilize these names, we must mention them in backticks.\nTibble only recycles the vectors with a length of 1.\nTibble can never generate the names of rows.\n\nsource: https://www.educative.io/answers/what-is-tibble-versus-data-frame-in-r\nWe can use following functions readr package to import tabular data into R as tibble:\n\n\nFigure 4: read() Functions\n\n\n\nread_csv() and read_tsv() are special cases of the more general read_delim(). They’re useful for reading the most common types of flat file data, comma separated values and tab separated values, respectively. read_csv2() uses ⁠;⁠ for the field separator and ⁠,⁠ for the decimal point. This format is common in some European countries.\nFor example, we will use read_csv() to import CSV file and see use glimpse() functions of dplyr package to explore the file structure.\n\nread_csv()read.csv()\n\n\n\n\nCode\n# define data location\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\ndf.chem_01<-readr::read_csv(paste0(dataFolder,\"PAHdata.csv\"))\n\nRows: 20 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Subject\ndbl (22): Napthalene, 1-Methyl Napthalene, 2-Methyl Napthalene, Acenapthylen...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nCode\ndplyr::glimpse(df.chem_01)\n\nRows: 20 Columns: 23 $ Subject  “P1”, “P3”, “P4”, “P5”, “P6”, “P7”, “P8”, … $ Napthalene  0.8993, 3.6257, 3.3921, 3.5772, 4.4907, NA… $ 1-Methyl Napthalene  4.9681, 4.6941, 3.5386, 4.7475, 5.1147, NA… $ 2-Methyl Napthalene  2.1508, 3.9316, 1.6955, 2.9361, 3.9976, NA… $ Acenapthylene  0.0131, 3.0151, 1.3859, 3.3943, 6.6593, NA… $ 1,2 Dimethyl napthalene  NA, NA, 1.2389, 2.6427, 2.1442, NA, 0.3623… $ 1,6 Dimethyl Napthalene  0.7003, 2.6382, 1.3807, 1.1006, 2.2575, NA… $ Fluorene  2.2481, 7.3490, 7.1567, 8.4422, 9.2363, NA… $ 1,6,7 Trimethylnapthalene  5.1024, 6.7913, 6.5171, 4.6803, 6.4649, NA… $ Anthracene  10.1656, 9.6419, 22.3997, 26.3787, 20.9594… $ Dibenzothiopene  1.1633, 4.1160, 4.2256, 3.9885, 3.2560, NA… $ 2-Methyl Anthracene  0.5409, 4.5190, 8.4014, 13.0101, 4.4900, N… $ 1-Methylphenanthrene  14.9581, 12.0937, 19.4927, 11.2138, 2.0336… $ 2-Methylphenanthrene  5.4785, 18.2456, 36.4282, 16.4553, 10.8040… $ Pyrene  4.8498, 14.9369, 10.1099, 26.0579, 20.8687… $ Fluoranthene  4.4798, 9.7189, 9.8037, 19.0489, 20.0866, … $ 1-Phenyl napthalene  2.8778, 6.2493, 5.2998, 7.9514, 10.1570, N… $ 2-Phenyl napthalene  3.4092, 8.7412, 3.6956, 12.8510, 15.1037, … $ 1 Methylpyrene  4.5763, 7.5114, 13.6010, 8.1125, 19.2354, … $ Benzo(c)phenanthrene  3.6456, 7.0372, 5.0960, 3.3828, 8.1571, NA… $ Triphenylene/Chrysene  1.7422, 5.1389, 3.1635, 5.7081, 6.7483, NA… $ Benz(a)pyrene  NA, 2.8455, NA, 5.0701, 0.5873, NA, 9.0914… $ Benz(e)pyrene  NA, 1.8163, 0.2980, 0.6617, 2.3666, NA, 2.…\n\n\n\n\n\nCode\n# define data location\ndataFolder<-\"E:/Dropbox/GitHub/Data/\"\ndf.chem_02<-read.csv(paste0(dataFolder,\"PAHdata.csv\"))\ndplyr::glimpse(df.chem_02)\n\n\nRows: 20\nColumns: 23\n$ Subject                    <chr> \"P1\", \"P3\", \"P4\", \"P5\", \"P6\", \"P7\", \"P8\", \"…\n$ Napthalene                 <dbl> 0.8993, 3.6257, 3.3921, 3.5772, 4.4907, NA,…\n$ X1.Methyl.Napthalene       <dbl> 4.9681, 4.6941, 3.5386, 4.7475, 5.1147, NA,…\n$ X2.Methyl.Napthalene       <dbl> 2.1508, 3.9316, 1.6955, 2.9361, 3.9976, NA,…\n$ Acenapthylene              <dbl> 0.0131, 3.0151, 1.3859, 3.3943, 6.6593, NA,…\n$ X1.2.Dimethyl.napthalene   <dbl> NA, NA, 1.2389, 2.6427, 2.1442, NA, 0.3623,…\n$ X1.6.Dimethyl.Napthalene   <dbl> 0.7003, 2.6382, 1.3807, 1.1006, 2.2575, NA,…\n$ Fluorene                   <dbl> 2.2481, 7.3490, 7.1567, 8.4422, 9.2363, NA,…\n$ X1.6.7.Trimethylnapthalene <dbl> 5.1024, 6.7913, 6.5171, 4.6803, 6.4649, NA,…\n$ Anthracene                 <dbl> 10.1656, 9.6419, 22.3997, 26.3787, 20.9594,…\n$ Dibenzothiopene            <dbl> 1.1633, 4.1160, 4.2256, 3.9885, 3.2560, NA,…\n$ X2.Methyl.Anthracene       <dbl> 0.5409, 4.5190, 8.4014, 13.0101, 4.4900, NA…\n$ X1.Methylphenanthrene      <dbl> 14.9581, 12.0937, 19.4927, 11.2138, 2.0336,…\n$ X2.Methylphenanthrene      <dbl> 5.4785, 18.2456, 36.4282, 16.4553, 10.8040,…\n$ Pyrene                     <dbl> 4.8498, 14.9369, 10.1099, 26.0579, 20.8687,…\n$ Fluoranthene               <dbl> 4.4798, 9.7189, 9.8037, 19.0489, 20.0866, N…\n$ X1.Phenyl.napthalene       <dbl> 2.8778, 6.2493, 5.2998, 7.9514, 10.1570, NA…\n$ X2.Phenyl.napthalene       <dbl> 3.4092, 8.7412, 3.6956, 12.8510, 15.1037, N…\n$ X1.Methylpyrene            <dbl> 4.5763, 7.5114, 13.6010, 8.1125, 19.2354, N…\n$ Benzo.c.phenanthrene       <dbl> 3.6456, 7.0372, 5.0960, 3.3828, 8.1571, NA,…\n$ Triphenylene.Chrysene      <dbl> 1.7422, 5.1389, 3.1635, 5.7081, 6.7483, NA,…\n$ Benz.a.pyrene              <dbl> NA, 2.8455, NA, 5.0701, 0.5873, NA, 9.0914,…\n$ Benz.e.pyrene              <dbl> NA, 1.8163, 0.2980, 0.6617, 2.3666, NA, 2.6…\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nglimps() of dplyr is a improved function of r-base str() function.\n\n\n\n\n\nThe write()⁠ family functions of are an improvement to analogous function such as write.csv() because they are approximately twice as fast. Unlike write.csv(), these functions do not include row names as a column in the written file. A generic function, output_column(), is applied to each variable to coerce columns to suitable output.\nWe can use following functions readr package to extort tabular data from R:\n\n\nFigure 5: write() Functions\n\n\n\n\n\nCode\nreadr::write_csv(df.chem_02, \"df.chem_02\")\n\n\nWe can also use as_tibble() function of tibble package\n\n\nCode\ndf.chem_03<-tibble::as_tibble(read.csv(paste0(dataFolder,\"PAHdata.csv\"), check.names = FALSE))\nstr(df.chem_03)\n\n\ntibble [20 × 23] (S3: tbl_df/tbl/data.frame)\n $ Subject                  : chr [1:20] \"P1\" \"P3\" \"P4\" \"P5\" ...\n $ Napthalene               : num [1:20] 0.899 3.626 3.392 3.577 4.491 ...\n $ 1-Methyl Napthalene      : num [1:20] 4.97 4.69 3.54 4.75 5.11 ...\n $ 2-Methyl Napthalene      : num [1:20] 2.15 3.93 1.7 2.94 4 ...\n $ Acenapthylene            : num [1:20] 0.0131 3.0151 1.3859 3.3943 6.6593 ...\n $ 1,2 Dimethyl napthalene  : num [1:20] NA NA 1.24 2.64 2.14 ...\n $ 1,6 Dimethyl Napthalene  : num [1:20] 0.7 2.64 1.38 1.1 2.26 ...\n $ Fluorene                 : num [1:20] 2.25 7.35 7.16 8.44 9.24 ...\n $ 1,6,7 Trimethylnapthalene: num [1:20] 5.1 6.79 6.52 4.68 6.46 ...\n $ Anthracene               : num [1:20] 10.17 9.64 22.4 26.38 20.96 ...\n $ Dibenzothiopene          : num [1:20] 1.16 4.12 4.23 3.99 3.26 ...\n $ 2-Methyl Anthracene      : num [1:20] 0.541 4.519 8.401 13.01 4.49 ...\n $ 1-Methylphenanthrene     : num [1:20] 14.96 12.09 19.49 11.21 2.03 ...\n $ 2-Methylphenanthrene     : num [1:20] 5.48 18.25 36.43 16.46 10.8 ...\n $ Pyrene                   : num [1:20] 4.85 14.94 10.11 26.06 20.87 ...\n $ Fluoranthene             : num [1:20] 4.48 9.72 9.8 19.05 20.09 ...\n $ 1-Phenyl napthalene      : num [1:20] 2.88 6.25 5.3 7.95 10.16 ...\n $ 2-Phenyl napthalene      : num [1:20] 3.41 8.74 3.7 12.85 15.1 ...\n $ 1 Methylpyrene           : num [1:20] 4.58 7.51 13.6 8.11 19.24 ...\n $ Benzo(c)phenanthrene     : num [1:20] 3.65 7.04 5.1 3.38 8.16 ...\n $ Triphenylene/Chrysene    : num [1:20] 1.74 5.14 3.16 5.71 6.75 ...\n $ Benz(a)pyrene            : num [1:20] NA 2.845 NA 5.07 0.587 ...\n $ Benz(e)pyrene            : num [1:20] NA 1.816 0.298 0.662 2.367 ...\n\n\n\n\n\n\n\nHow do I read data into R?\nR Coder\nIntroduction to bioinformatics\nMany Ways of Reading Data Into R — 1\nreadr\nfeather\ndata.tabler\nforeign\nhaven"
  },
  {
    "objectID": "data-wrgaling-basic.html",
    "href": "data-wrgaling-basic.html",
    "title": "Data Science with R For Envrionmental Modeling",
    "section": "",
    "text": "Introduction to Data Wrangling\nData Wrangling, also known as data munging, is the process of cleaning, transforming, and mapping data from one format to another to make it suitable for analysis and visualization. It involves resolving issues such as missing or duplicate values, inconsistent formatting, and dealing with outliers. Data wrangling is a crucial step in the data science process as it ensures that the data is reliable and trustworthy for further analysis and modeling. Data Wrangling of environmental data is the very important steps for building geospatial and machine Learning models.\n\nSteps of Data Wrangling\nHere below 6 steps of data wrangling:\n\nDiscovering: systematic wrangling based on some criteria which could restrict and divide the data accordingly.\nStructuring: the raw data should be restructured to suit the analytically method. Feature engineering can be done in this stage,\nCleaning: outliers and missing values identification, transformation and imputation\nEnriching upscale, downsample, or perform data augmentation.\nValidating: validation data after processing.\nPublishing: process for further use\n\n\n\nFigure 1: Six steps of Data Wrangling\n\n\n\n\n\nImportant R Packages for Data Wrangling\n\ndplyr\npurr\ntidyr\nlubridate\nmagrittr\nplyr\npurrrlyr\ndata.table\nsqldf\ndtplyr\nDT\njanitor"
  },
  {
    "objectID": "data-wrgaling-dplyr.html",
    "href": "data-wrgaling-dplyr.html",
    "title": "Data Science with R For Envrionmental Modeling",
    "section": "",
    "text": "Data Wrangling with dplyr and tidyr\nIn section you will learn you to wrangle data with two popular R-packages tidyr and dplyr and both are come with tidyverse, an collection of several R packages designed for data science.\n\ntidyr - Package\n\ntidyr helps to create tidy data that enables you to work with tidy data formats which are easy to manipulate, model and visualize, and have a specific structure. It follows a set of rules for organizing variables, values, and observations into tables, with each column representing a variable and each row representing an observation. Tidy data makes it easier to perform data analysis and visualization and is the preferred format for many data analysis tools and techniques.\ntidyr functions fall into five main categories:\n\nPivotting which converts between long(pivot_longer()) and wide forms (pivot_wider()), replacing\nRectangling, which turns deeply nested lists (as from JSON) into tidy tibbles.\nNesting converts grouped data to a form where each group becomes a single row containing a nested data frame\nSplitting and combining character columns. Use separate() and extract() to pull a single character column into multiple columns;\nMake implicit missing values explicit with complete(); make explicit missing values implicit with drop_na(); replace missing values with next/previous value with fill(), or a known value with replace_na().\n\n\n\ndplyr - Package\n\ndplyr is part of a larger [tidyverse]( https://www.tidyverse.org/. It provides a grammar for data manipulation and a set of functions to clean, process, and aggregate data efficiently. Some of the key features of dplyr include:\nA tibble data structure, which is similar to a data frame but is designed to be more efficient and easier to work with. A set of verbs for data manipulation, including filter() for subsetting rows, arrange() for sorting rows, select() for selecting columns, mutate() for adding new columns, and summarize() for aggregating data. A chainable syntax with pipe (%>%) that makes it easy to perform multiple operations in a single line of code.Support for working with remote data sources, including databases and big data systems. Overall, dplyr is a popular and widely used package for data manipulation in R and is known for its simplicity and ease of use.\nIn addition to data frames/tibbles, dplyr makes working with following packages:\ndtplyr: for large, in-memory datasets. Translates your dplyr code to high performance data.table code.\ndbplyr: for data stored in a relational database. Translates your dplyr code to SQL.\nsparklyr: for very large datasets stored in Apache Spark.\n\nCheat-sheet\nHere below data Wrangling with dplyr and tidyr Cheat Sheets:\n\n\nFigure 1: Data Wragling with dplyrand tidyr Cheat sheet-2\n\n\n\n{fig-dplyr_tidyr_02}\nIn addition to tidyr, and dplyr, there are five packages (including stringr and forcats) which are designed to work with specific types of data:\n\nlubridate for dates and date-times.\nhms for time-of-day values.\nblob for storing blob (binary) data\n\n\n\n\nInstall tidyverse\nYou can install the tidyverse with a following line of code:\n\ninstall.packages(“tidyverse”)\n\n\n\nLoad Package\n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nData\nIn this exercise we will use following CSV files:\n\nusa_division.csv: USA division names with IDs\nusa_state.csv: USA State names with ID and division ID.\nusa_corn_production.csv USA grain crop production by state from 2012-2022\ngp_soil_data.csv: Soil carbon with co-variate from four states in the Greatplain region in the USA\nusa_geochemical_raw.csv: Soil geochemical data for the USA, but cleaned\n\nWe will use read_csv() function of readr package to import data as a Tidy data.\n\n\nCode\n# Create a data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/USA/\"\n# Load data\ndiv<-read_csv(paste0(dataFolder, \"usa_division.csv\"))\nstate<-read_csv(paste0(dataFolder, \"usa_state.csv\"))\ncorn<-read_csv(paste0(dataFolder, \"usa_corn_production.csv\"))\nsoil<-read_csv(paste0(dataFolder, \"gp_soil_data.csv\"))\n\n\n\n\nPipe Operator\nBefore starting this tutorial, I like to brief you about the Pipe Operator. This is the most important operator for data wrangling in R. The pipe operator, written as %>%, has been a longstanding feature of the magrittr package for R. It takes the output of one function and passes it into another function as an argument. This allows us to link a sequence of analysis steps. In this tutorial, you can see exactly how this works in a real example. (Source:https://towardsdatascience.com/an-introduction-to-the-pipe-in-r-823090760d64)\n\n\nSome Important Functions\n\nJoin\nIn R we use base::merge() function to merge two dataframes. This function is present inside join() function of dplyr package. The most important condition for joining two dataframes is that the column type should be the same of “key” variable by the merging happens. Types of base::merge() and several join() function of dplyr available in R are:\n{fig-dplyr_join}\ninner_join() is a function in the dplyr library that performs an inner join on two data frames. An inner join returns only the rows that have matching values in both data frames. If there is no match in one of the data frames for a row in the other data frame, the result will not contain that row.\n\ninner_join(x, y, …..)\n\nWe will join state, division and USA corn production data one by one e using inner_join()* function;\n\n\nCode\ncorn_state = dplyr::inner_join(corn, state) \n\n\n\n\nCode\ncorn_state_div = dplyr::inner_join(corn_state, div)  \n\n\nWe can run multiple inner_join() functions in a series with pipe %>% function:\n\n\nCode\nmf.usa = dplyr::inner_join(corn, state) %>% \n     dplyr::inner_join(div) %>%\n  glimpse()\n\n\nRows: 465\nColumns: 6\n$ STATE_ID      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ YEAR          <dbl> 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 20…\n$ MT            <dbl> 734352.8, 1101529.2, 1151061.8, 914829.3, 960170.7, 9968…\n$ STATE_NAME    <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"…\n$ DIVISION_ID   <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ DIVISION_NAME <chr> \"East South Central\", \"East South Central\", \"East South …\n\n\n\n\nRelocate\nNow we will will organize DIVISION_FIPS, DIVISION_NAME, STATE_FIPS, STATE_NAME, DIVISION_NAME, YEAR, MT will use relocate() function:\n\nrelocate(.data, …, .before = NULL, .after = NULL)\n\n\n\nCode\nmf.usa<-dplyr::relocate(mf.usa, DIVISION_ID, DIVISION_NAME, STATE_ID,   STATE_NAME, YEAR, MT,  \n                    .after =  DIVISION_ID) \nhead(mf.usa)\n\n\n# A tibble: 6 × 6\n  DIVISION_ID DIVISION_NAME      STATE_ID STATE_NAME  YEAR       MT\n        <dbl> <chr>                 <dbl> <chr>      <dbl>    <dbl>\n1           2 East South Central        1 Alabama     2012  734353.\n2           2 East South Central        1 Alabama     2013 1101529.\n3           2 East South Central        1 Alabama     2014 1151062.\n4           2 East South Central        1 Alabama     2015  914829.\n5           2 East South Central        1 Alabama     2016  960171.\n6           2 East South Central        1 Alabama     2017  996876.\n\n\nNow explore regions names with levels()\n\n\nCode\nlevels(as.factor(mf.usa$DIVISION_NAME))\n\n\n[1] \"East North Central\" \"East South Central\" \"Middle Atlantic\"   \n[4] \"Mountain\"           \"New England\"        \"Pacific\"           \n[7] \"South Atlantic\"     \"West North Central\" \"West South Central\"\n\n\n\n\nRename\nThe rename() function can be used to rename variables. We will rename STAT_ID to SATE_FIPS.\n\nrename_with(.data, … .cols = …)\n\n\n\nCode\ndf.usa <- mf.usa %>% \n        dplyr::rename(\"STATE_FIPS\" = \"STATE_ID\")\nnames(df.usa)\n\n\n[1] \"DIVISION_ID\"   \"DIVISION_NAME\" \"STATE_FIPS\"    \"STATE_NAME\"   \n[5] \"YEAR\"          \"MT\"           \n\n\n\n\nPipe Join, Relocate and Rename functions\nWe can also all together inner_join(), relocate(), and rename() in a single line with pipe:\n\n\nCode\ndf.corn = dplyr::inner_join(corn, state) %>% \n          dplyr::inner_join(div) %>%\n          dplyr::relocate(DIVISION_ID, DIVISION_NAME, STATE_ID,   STATE_NAME, YEAR, MT,  \n                    .after =  DIVISION_ID) %>%\n          dplyr::rename(\"STATE_FIPS\" = \"STATE_ID\") %>%\n        glimpse()\n\n\nRows: 465\nColumns: 6\n$ DIVISION_ID   <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ DIVISION_NAME <chr> \"East South Central\", \"East South Central\", \"East South …\n$ STATE_FIPS    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ STATE_NAME    <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"…\n$ YEAR          <dbl> 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 20…\n$ MT            <dbl> 734352.8, 1101529.2, 1151061.8, 914829.3, 960170.7, 9968…\n\n\n\n\nSelect\ndplyr::select() is used to extract a subset of columns from a data frame. It selects specific columns by name or by position. It is part of the dplyr package, which provides a set of functions that perform common data manipulation tasks efficiently and effectively.\n\nselect(.data, …)\n\nNow we will use select() to create a dataframe with state names, year and production\n\n\nCode\ndf.state <- df.corn %>% \n           dplyr::select(STATE_NAME, YEAR,  MT,) %>%\n           glimpse()\n\n\nRows: 465\nColumns: 3\n$ STATE_NAME <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Ala…\n$ YEAR       <dbl> 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021,…\n$ MT         <dbl> 734352.8, 1101529.2, 1151061.8, 914829.3, 960170.7, 996875.…\n\n\n\n\nFilter\nThe filter() function in dplyr is used to subset a data frame based on some condition(s) specified in the filter argument. The syntax for using filter is:\n\nfilter(df, condition1, condition2, …)\n\nWe will use filter() to extract data only for northeast regions (single criteria) from us.df. This code will return a filtered data frame containing only the rows where the DIVISION_NAME column is equal to “East North Central”.\n\n\nCode\ndf.01<-df.corn %>% \n              dplyr::filter(DIVISION_NAME == \"East North Central\")\nlevels(as.factor(df.01$STATE_NAME))\n\n\n[1] \"Illinois\"  \"Indiana\"   \"Michigan\"  \"Ohio\"      \"Wisconsin\"\n\n\nFiltering by multiple criteria within a single logical expression - select data from East North Central, South Central and Middle Atlantic Division\n\n\nCode\ndf.02<- df.corn %>% \n            dplyr::filter(DIVISION_NAME %in%c(\"East North Central\",\"East South Central\", \"Middle Atlantic\")) \nlevels(as.factor(df.02$STATE_NAME))\n\n\n [1] \"Alabama\"      \"Illinois\"     \"Indiana\"      \"Kentucky\"     \"Michigan\"    \n [6] \"Mississippi\"  \"New Jersey\"   \"New York\"     \"Ohio\"         \"Pennsylvania\"\n[11] \"Tennessee\"    \"Wisconsin\"   \n\n\nor we can use | which represents OR in the logical condition, any of the two conditions.\n\n\nCode\ndf.03<- df.usa %>% \n                    dplyr::filter(DIVISION_NAME == \"East North Central\" | DIVISION_NAME == \"Middle Atlantic\")\nlevels(as.factor(df.03$STATE_NAME))\n\n\n[1] \"Illinois\"     \"Indiana\"      \"Michigan\"     \"New Jersey\"   \"New York\"    \n[6] \"Ohio\"         \"Pennsylvania\" \"Wisconsin\"   \n\n\nFollowing filter create a files for the Middle Atlantic Division only with New York state.\n\n\nCode\ndf.ny<-df.corn %>% \n         dplyr::filter(DIVISION_NAME == \"Middle Atlantic\" & STATE_NAME == \"New York\")\nhead(df.ny)\n\n\n# A tibble: 6 × 6\n  DIVISION_ID DIVISION_NAME   STATE_FIPS STATE_NAME  YEAR       MT\n        <dbl> <chr>                <dbl> <chr>      <dbl>    <dbl>\n1           3 Middle Atlantic         36 New York    2012 2314570.\n2           3 Middle Atlantic         36 New York    2013 2401189.\n3           3 Middle Atlantic         36 New York    2014 2556391 \n4           3 Middle Atlantic         36 New York    2015 2143111.\n5           3 Middle Atlantic         36 New York    2016 1867761.\n6           3 Middle Atlantic         36 New York    2017 1983464.\n\n\nFollowing filters will select State where corn production (MT) is greater than the global average of production\n\n\nCode\nmean.prod <- df.corn %>% \n              dplyr::filter(MT > mean(MT, na.rm = TRUE))\nlevels(as.factor(mean.prod$STATE_NAME))\n\n\n [1] \"Illinois\"     \"Indiana\"      \"Iowa\"         \"Kansas\"       \"Michigan\"    \n [6] \"Minnesota\"    \"Missouri\"     \"Nebraska\"     \"North Dakota\" \"Ohio\"        \n[11] \"South Dakota\" \"Wisconsin\"   \n\n\nWe use will & in the following filters to select states or rows where MT is greater than the global average of for the year 2017\n\n\nCode\nmean.prod.2017 <- df.corn %>% \n              dplyr::filter(MT > mean(MT, na.rm = TRUE) & YEAR ==2017)\nlevels(as.factor(mean.prod.2017$STATE_NAME))\n\n\n [1] \"Illinois\"     \"Indiana\"      \"Iowa\"         \"Kansas\"       \"Minnesota\"   \n [6] \"Missouri\"     \"Nebraska\"     \"North Dakota\" \"Ohio\"         \"South Dakota\"\n[11] \"Wisconsin\"   \n\n\nFollowing command will select counties starting with “A”. filter() with grepl() is used to search for pattern matching.\n\n\nCode\nstate.a <- df.corn %>% \n          dplyr::filter(grepl(\"^A\", STATE_NAME))\nlevels(as.factor(state.a $STATE_NAME))\n\n\n[1] \"Alabama\"  \"Arizona\"  \"Arkansas\"\n\n\n\n\nSummarize\nSummarize is a function in the dplyr used to collapse multiple values in a data frame into a single summary value. The function takes a data frame as input and returns a smaller data frame with summary statistics, such as mean, sum, count, etc. It can be used with other dplyr functions to manipulate and analyze data.\n\nsummarise(.data, …, .groups = NULL)\n\n\nCenter: mean(), median()\nSpread: sd(), IQR(), mad()\nRange: min(), max(), quantile()\nPosition: first(), last(), nth(),\nCount: n(), n_distinct()\nLogical: any(), all()\n\nsummarise() and summarize() are synonyms.\n\n\nCode\n# mean\nsummarize(df.corn, Mean=mean(MT))\n\n\n# A tibble: 1 × 1\n      Mean\n     <dbl>\n1 8360402.\n\n\nCode\n# median\nsummarise(df.corn, Median=median(MT))\n\n\n# A tibble: 1 × 1\n    Median\n     <dbl>\n1 2072749.\n\n\nThe scoped variants (_if, _at, _all) of summarise() make it easy to apply the same transformation to multiple variables. There are three variants.\n\nsummarise_at() affects variables selected with a character vector or vars()\nsummarise_all() affects every variable\nsummarise_if() affects variables selected with a predicate function\n\nFollowing summarise_at() function mean of SOC from USA soil data (soil).\n\n\nCode\nsoil %>%\n    dplyr::summarise_at(\"SOC\", mean, na.rm = TRUE)\n\n\n# A tibble: 1 × 1\n    SOC\n  <dbl>\n1  6.35\n\n\nFor multiple variables:\n\n\nCode\nsoil %>%\n    dplyr::summarise_at(c(\"SOC\", \"NDVI\"), mean, na.rm = TRUE)\n\n\n# A tibble: 1 × 2\n    SOC  NDVI\n  <dbl> <dbl>\n1  6.35 0.437\n\n\nThe summarise_if() variants apply a predicate function (a function that returns TRUE or FALSE) to determine the relevant subset of columns.\nHere we apply mean() to the numeric columns:\n\n\nCode\nsoil %>%\n    dplyr::summarise_if(is.numeric, mean, na.rm = TRUE)\n\n\n# A tibble: 1 × 15\n     ID   FIPS STATE_ID Longi…¹ Latit…²   SOC   DEM Aspect Slope     TPI KFactor\n  <dbl>  <dbl>    <dbl>   <dbl>   <dbl> <dbl> <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n1  238. 29151.     29.1   -104.    38.9  6.35 1632.   165.  4.84 0.00937   0.256\n# … with 4 more variables: MAP <dbl>, MAT <dbl>, NDVI <dbl>, SiltClay <dbl>,\n#   and abbreviated variable names ¹​Longitude, ²​Latitude\n\n\n\n\nCode\nsoil %>%\n   dplyr::summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))\n\n\n# A tibble: 1 × 15\n     ID   FIPS STATE_ID Longi…¹ Latit…²   SOC   DEM Aspect Slope     TPI KFactor\n  <dbl>  <dbl>    <dbl>   <dbl>   <dbl> <dbl> <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n1  238. 29151.     29.1   -104.    38.9  6.35 1632.   165.  4.84 0.00937   0.256\n# … with 4 more variables: MAP <dbl>, MAT <dbl>, NDVI <dbl>, SiltClay <dbl>,\n#   and abbreviated variable names ¹​Longitude, ²​Latitude\n\n\nIt is better to select first our target numerical columns and then apply summarise_all():\n\n\nCode\nsoil %>%\n    # First select  numerical columns\n    dplyr::select(SOC, DEM, NDVI, MAP, MAT) %>%\n    # get mean of all these variables\n    dplyr::summarise_all(mean, na.rm = TRUE)\n\n\n# A tibble: 1 × 5\n    SOC   DEM  NDVI   MAP   MAT\n  <dbl> <dbl> <dbl> <dbl> <dbl>\n1  6.35 1632. 0.437  501.  8.88\n\n\n\n\nMutate\n*mutate() adds new columns to a data frame based on existing columns or variables. The new columns are specified by expressions that use the values from one or more existing columns. The function returns a new data frame with the added columns and the same number of rows as the original data frame.\n\nmutate(.data, …, .dots = NULL)\n\nIn this exercise we will create a new column (MT_1000) in df.corn dataframe dividing MT column by 1000\n\n\nCode\ndf.corn %>%\n    # get mean of all these variables\n    dplyr::mutate(MT_1000 = MT / 10000) %>%\n    glimpse()\n\n\nRows: 465\nColumns: 7\n$ DIVISION_ID   <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ DIVISION_NAME <chr> \"East South Central\", \"East South Central\", \"East South …\n$ STATE_FIPS    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ STATE_NAME    <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"…\n$ YEAR          <dbl> 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 20…\n$ MT            <dbl> 734352.8, 1101529.2, 1151061.8, 914829.3, 960170.7, 9968…\n$ MT_1000       <dbl> 73.43528, 110.15292, 115.10618, 91.48293, 96.01707, 99.6…\n\n\n\n\nGroup by\ngroup_by() allows you to group a data frame by one or multiple variables. After grouping, you can perform operations on the grouped data, such as aggregating with summarize(), transforming with mutate(), or filtering with filter(). The result of grouping is a grouped tibble, which is a data structure that retains the grouping structure and allows you to perform further operations on the grouped data.\n\ngroup_by(.data, …, )\n\nWe can calculate global mean of USA corn production by division:\n\n\nCode\ndf.corn %>% \n          dplyr::group_by(DIVISION_NAME) %>%\n          dplyr::summarize(Prod_MT = mean(MT)) \n\n\n# A tibble: 9 × 2\n  DIVISION_NAME        Prod_MT\n  <chr>                  <dbl>\n1 East North Central 22246702.\n2 East South Central  3143670.\n3 Middle Atlantic     2036263.\n4 Mountain             712629.\n5 New England           15203.\n6 Pacific              384083.\n7 South Atlantic      1163065.\n8 West North Central 27893388.\n9 West South Central  3163740.\n\n\nWe can also apply the group_by(), summarize() and mutate() functions with pipe to calculate mean of corn production in 1000 MT by division for the year 2022\n\n\nCode\ndf.corn %>% \n          dplyr::filter(YEAR==2020) %>%\n          dplyr::group_by(DIVISION_NAME) %>%\n          dplyr::summarize(Prod_MT = mean(MT)) %>%\n          dplyr::mutate(Prod_1000_MT = Prod_MT / 1000) \n\n\n# A tibble: 8 × 3\n  DIVISION_NAME        Prod_MT Prod_1000_MT\n  <chr>                  <dbl>        <dbl>\n1 East North Central 22746952.       22747.\n2 East South Central  3350119.        3350.\n3 Middle Atlantic     1929554.        1930.\n4 Mountain             651221.         651.\n5 Pacific              391731.         392.\n6 South Atlantic      1223075.        1223.\n7 West North Central 28281091.       28281.\n8 West South Central  3009964.        3010.\n\n\nWe can also apply the group_by() and summarize() functions to calculate statistic of multiple variable:\n\n\nCode\nsoil %>% \n  group_by(STATE) %>%\n  summarize(SOC = mean(SOC),\n            NDVI = mean(NDVI),\n            MAP = mean(MAP),\n            MAT = mean(MAT))\n\n\n# A tibble: 4 × 5\n  STATE        SOC  NDVI   MAP   MAT\n  <chr>      <dbl> <dbl> <dbl> <dbl>\n1 Colorado    7.29 0.482  473.  6.93\n2 Kansas      7.43 0.570  742. 12.6 \n3 New Mexico  3.51 0.301  388. 11.6 \n4 Wyoming     6.90 0.390  419.  5.27\n\n\nFollowing code will idenfify State where corn production data has not reported for the all years.\n\n\nCode\ndf.corn %>% \n  dplyr::group_by(STATE_NAME) %>% \n  dplyr::summarise(n = n()) %>%\n  dplyr::filter(n < 11) \n\n\n# A tibble: 7 × 2\n  STATE_NAME        n\n  <chr>         <int>\n1 Connecticut       2\n2 Maine             2\n3 Massachusetts     2\n4 Nevada            2\n5 New Hampshire     2\n6 Rhode Island      2\n7 Vermont           2\n\n\nCode\n# Get the state names\n\n\n\n\nPivoting Data-frame\nPivoting a data frame in R involves transforming columns into rows and vice versa. In R, there are multiple ways to pivot a data frame, but the most common methods are:\ntidyr::pivot_wider: The pivot_wider() function is used to reshape a data frame from a long format to a wide format, in which each row becomes a variable, and each column is an observation.\ntidyr::pivot_longer: This is a relatively new function in the tidyr library that makes it easy to pivot a data frame from wide to long format. It is used to reshape a data frame from a wide format to a long format, in which each column becomes a variable, and each row is an observation.\ntidyr::spread(): This function is also used to pivot data from long to wide format. It creates new columns from the values of one column, based on the values of another column.\ntidy::gather() : This function is used to pivot data from wide to long format. It collects values of multiple columns into a single column, based on the values of another column.\n\npivot_wider\npivot_wider() convert a dataset wider by increasing the number of columns and decreasing the number of rows.\n\n\nCode\ncorn.wider = df.corn  %>% \n           dplyr::select (STATE_FIPS,STATE_NAME, YEAR, MT) %>%\n           # Drop state where reporting years less than 11 \n           dplyr::filter(STATE_NAME!= 'Connecticut', \n                        STATE_NAME!= 'Maine',                        \n                        STATE_NAME!= 'Massachusetts', \n                        STATE_NAME!= 'Nevada',\n                        STATE_NAME!= 'New Hampshire', \n                        STATE_NAME!= 'Rhode Island', \n                        STATE_NAME!= 'Vermont') %>%\n           tidyr::pivot_wider(names_from = YEAR, values_from = MT) %>%\n           glimpse()\n\n\nRows: 41\nColumns: 13\n$ STATE_FIPS <dbl> 1, 4, 5, 6, 8, 10, 12, 13, 16, 17, 18, 19, 20, 21, 22, 24, …\n$ STATE_NAME <chr> \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\",…\n$ `2012`     <dbl> 734352.8, 158504.4, 3142399.9, 823003.5, 3412162.2, 610394.…\n$ `2013`     <dbl> 1101529.2, 229297.9, 4110445.0, 873298.1, 3261024.2, 733692…\n$ `2014`     <dbl> 1151061.8, 149359.9, 2517526.9, 398166.0, 3745681.8, 853485…\n$ `2015`     <dbl> 914829.3, 192034.1, 2045951.0, 239280.6, 3426640.9, 799837.…\n$ `2016`     <dbl> 960170.7, 273064.4, 3236003.9, 469924.8, 4071581.0, 708189.…\n$ `2017`     <dbl> 996875.6, 158504.4, 2765825.0, 339361.9, 4722109.3, 820946.…\n$ `2018`     <dbl> 970839.3, 111765.9, 2965479.6, 285638.1, 3929587.5, 611410.…\n$ `2019`     <dbl> 1138869.1, 217105.3, 3267247.5, 256045.5, 4061674.5, 736130…\n$ `2020`     <dbl> 1284291.8, 148801.1, 2827677.3, 285003.1, 3123348.9, 715301…\n$ `2021`     <dbl> 1407742.3, 82757.6, 3879292.8, 238772.6, 3768289.0, 803901.…\n$ `2022`     <dbl> 869233.9, 223531.8, 3054130.3, 89920.8, 3012091.0, 721144.1…\n\n\n\n\npivot_longer\nThe pivot_longer() function can be used to pivot a data frame from a wide format to a long format.\n\n\nCode\nnames(corn.wider)\n\n\n [1] \"STATE_FIPS\" \"STATE_NAME\" \"2012\"       \"2013\"       \"2014\"      \n [6] \"2015\"       \"2016\"       \"2017\"       \"2018\"       \"2019\"      \n[11] \"2020\"       \"2021\"       \"2022\"      \n\n\n\n\nCode\ncorn.longer<- corn.wider %>% \n               tidyr::pivot_longer(cols= c(\"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\",                                              \"2018\", \"2019\", \"2020\", \"2021\",  \"2022\"),\n                          names_to = \"YEAR\", # column need to be wider\n                          values_to = \"MT\") # data\nhead(corn.longer)\n\n\n# A tibble: 6 × 4\n  STATE_FIPS STATE_NAME YEAR        MT\n       <dbl> <chr>      <chr>    <dbl>\n1          1 Alabama    2012   734353.\n2          1 Alabama    2013  1101529.\n3          1 Alabama    2014  1151062.\n4          1 Alabama    2015   914829.\n5          1 Alabama    2016   960171.\n6          1 Alabama    2017   996876.\n\n\nFollowing command combined select(), rename(), summarize() and pivot_longer() the data:\n\n\nCode\nsoil %>% \n  # First select  numerical columns\n  dplyr::select(SOC, DEM, NDVI, MAP, MAT) %>%\n  # get summary statistics\n  dplyr::summarise_all(funs(min = min, \n                      q25 = quantile(., 0.25), \n                      median = median, \n                      q75 = quantile(., 0.75), \n                      max = max,\n                      mean = mean, \n                      sd = sd)) %>% \n  # create a nice looking table\n   tidyr::pivot_longer(everything(), names_sep = \"_\", names_to = c( \"variable\", \".value\")) \n\n\n# A tibble: 5 × 8\n  variable     min      q25   median      q75      max     mean      sd\n  <chr>      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>   <dbl>\n1 SOC        0.408    2.77     4.97     8.71    30.5      6.35    5.05 \n2 DEM      259.    1175.    1593.    2238.    3618.    1632.    770.   \n3 NDVI       0.142    0.308    0.417    0.557    0.797    0.437   0.162\n4 MAP      194.     354.     434.     591.    1128.     501.    207.   \n5 MAT       -0.591    5.87     9.17    12.4     16.9      8.88    4.10 \n\n\n\n\nSpread\ndplyr::spread() is equivalent to tidyr::pivot_wider():\n\n\nCode\ndf.corn %>% \n  dplyr::select (STATE_FIPS, YEAR, STATE_NAME, MT) %>%\n  tidyr::spread(YEAR, MT) \n\n\n# A tibble: 48 × 13\n   STATE…¹ STATE…² `2012`  `2013`  `2014`  `2015`  `2016` `2017`  `2018`  `2019`\n     <dbl> <chr>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>\n 1       1 Alabama 7.34e5  1.10e6  1.15e6  9.15e5  9.60e5 9.97e5  9.71e5  1.14e6\n 2       4 Arizona 1.59e5  2.29e5  1.49e5  1.92e5  2.73e5 1.59e5  1.12e5  2.17e5\n 3       5 Arkans… 3.14e6  4.11e6  2.52e6  2.05e6  3.24e6 2.77e6  2.97e6  3.27e6\n 4       6 Califo… 8.23e5  8.73e5  3.98e5  2.39e5  4.70e5 3.39e5  2.86e5  2.56e5\n 5       8 Colora… 3.41e6  3.26e6  3.75e6  3.43e6  4.07e6 4.72e6  3.93e6  4.06e6\n 6       9 Connec… 2.05e4 NA      NA      NA      NA      2.32e4 NA      NA     \n 7      10 Delawa… 6.10e5  7.34e5  8.53e5  8.00e5  7.08e5 8.21e5  6.11e5  7.36e5\n 8      12 Florida 1.17e5  2.64e5  1.37e5  1.79e5  1.47e5 1.51e5  2.47e5  2.21e5\n 9      13 Georgia 1.42e6  2.07e6  1.34e6  1.24e6  1.43e6 1.10e6  1.27e6  1.42e6\n10      16 Idaho   6.69e5  5.29e5  4.06e5  3.68e5  4.78e5 5.93e5  6.76e5  7.81e5\n# … with 38 more rows, 3 more variables: `2020` <dbl>, `2021` <dbl>,\n#   `2022` <dbl>, and abbreviated variable names ¹​STATE_FIPS, ²​STATE_NAME\n\n\n\n\nGather\ntidyr::gather(“key”, “value”, x, y, z) is equivalent to tidyr::pivot_longer()\n\n\nCode\ncorr.gathered<-corn.wider %>%\n        tidyr::gather(key = YEAR, value = MT, -STATE_FIPS, -STATE_NAME) %>%\n        glimpse()\n\n\nRows: 451\nColumns: 4\n$ STATE_FIPS <dbl> 1, 4, 5, 6, 8, 10, 12, 13, 16, 17, 18, 19, 20, 21, 22, 24, …\n$ STATE_NAME <chr> \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\",…\n$ YEAR       <chr> \"2012\", \"2012\", \"2012\", \"2012\", \"2012\", \"2012\", \"2012\", \"20…\n$ MT         <dbl> 734352.8, 158504.4, 3142399.9, 823003.5, 3412162.2, 610394.…\n\n\n\n\n\n\nCleaning a messy data\nIn this exercise you will learn how a clean data and make it ready for analysis, such as dealing with missing values, data with below detection limit and cleaning spatial characters. We will clean usa_geochemical_raw.csv data for further analysis. The data could be found here\n\n\nCode\n## Create a data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/USA/\"\ndf.geo<-read_csv(paste0(dataFolder,\"usa_geochemical_raw.csv\"))\nmeta.geo<-read_csv(paste0(dataFolder,\"usa_geochemical_meta_data.csv\"))\n\n\n\n\nCode\nglimpse(df.geo)\n\n\nRows: 4,857\nColumns: 56\n$ A_LabID    <chr> \"C-328943\", \"C-328929\", \"C-328930\", \"C-329034\", \"C-328968\",…\n$ SiteID     <dbl> 96, 208, 288, 656, 912, 1232, 1312, 1488, 1680, 1824, 2144,…\n$ StateID    <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\",…\n$ Latitude   <dbl> 31.3146, 33.8523, 31.2179, 33.0992, 34.5501, 34.3577, 31.06…\n$ Longitude  <dbl> -87.1166, -86.9041, -85.8788, -86.9976, -87.6779, -87.3130,…\n$ CollDate   <chr> \"2/18/2009\", \"2/26/2009\", \"2/18/2009\", \"2/23/2009\", \"2/25/2…\n$ LandCover1 <chr> \"Planted/Cultivated\", \"Developed\", \"Planted/Cultivated\", \"F…\n$ LandCover2 <chr> \"Pasture/Hay\", \"Low Intensity Residential\", \"Fallow\", \"Ever…\n$ A_Depth    <chr> \"0-20\", \"0-20\", \"0-20\", \"0-20\", \"0-5\", \"0-10\", \"0-25\", \"0-3…\n$ A_Ag       <chr> \"<1\", \"<1\", \"<1\", \"<1\", \"<1\", \"<1\", \"<1\", \"<1\", \"<1\", \"<1\",…\n$ A_Al       <chr> \"0.87\", \"1.15\", \"1.05\", \"4.17\", \"1.51\", \"2.78\", \"2.14\", \"1.…\n$ A_As       <chr> \"1.6\", \"1.5\", \"1.2\", \"3.7\", \"2.7\", \"4.3\", \"2.7\", \"3.7\", \"2.…\n$ A_Ba       <chr> \"49\", \"118\", \"34\", \"316\", \"179\", \"160\", \"50\", \"185\", \"77\", …\n$ A_Be       <chr> \"0.2\", \"0.4\", \"0.2\", \"1.1\", \"0.6\", \"0.5\", \"0.3\", \"0.5\", \"0.…\n$ A_Bi       <chr> \"0.08\", \"0.1\", \"0.07\", \"0.16\", \"0.09\", \"0.19\", \"0.16\", \"0.2…\n$ A_C_Tot    <chr> \"0.71\", \"0.88\", \"0.65\", \"1.14\", \"1.36\", \"1.71\", \"0.89\", \"4.…\n$ A_C_Inorg  <chr> \"N.D.\", \"N.D.\", \"N.D.\", \"N.D.\", \"N.D.\", \"N.D.\", \"N.D.\", \"N.…\n$ A_C_Org    <chr> \"0.71\", \"0.88\", \"0.65\", \"1.14\", \"1.36\", \"1.71\", \"0.89\", \"4.…\n$ A_Ca       <chr> \"0.07\", \"0.02\", \"0.05\", \"0.05\", \"0.03\", \"0.02\", \"0.04\", \"0.…\n$ A_Cd       <chr> \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"0.…\n$ A_Ce       <chr> \"25.2\", \"27.8\", \"42.2\", \"41.7\", \"61.7\", \"32.6\", \"48.6\", \"44…\n$ A_Co       <chr> \"1.2\", \"2.4\", \"1\", \"7\", \"6.1\", \"3.5\", \"1.5\", \"3.9\", \"1.3\", …\n$ A_Cr       <chr> \"11\", \"14\", \"8\", \"28\", \"17\", \"30\", \"17\", \"22\", \"22\", \"10\", …\n$ A_Cs       <chr> \"<5\", \"<5\", \"<5\", \"<5\", \"<5\", \"<5\", \"<5\", \"<5\", \"<5\", \"<5\",…\n$ A_Cu       <chr> \"4.4\", \"6.1\", \"10.9\", \"13.8\", \"5.2\", \"8.4\", \"15.1\", \"10.1\",…\n$ A_Fe       <chr> \"0.57\", \"0.54\", \"0.38\", \"2.38\", \"1.06\", \"1.66\", \"0.9\", \"1.1…\n$ A_Ga       <chr> \"1.96\", \"2.24\", \"1.98\", \"9.53\", \"3.29\", \"6.43\", \"4.22\", \"4.…\n$ A_Hg       <chr> \"0.01\", \"0.02\", \"<0.01\", \"0.03\", \"0.03\", \"0.05\", \"0.02\", \"0…\n$ A_In       <chr> \"<0.02\", \"<0.02\", \"<0.02\", \"0.03\", \"<0.02\", \"0.02\", \"<0.02\"…\n$ A_K        <chr> \"0.09\", \"0.25\", \"0.05\", \"0.89\", \"0.44\", \"0.46\", \"0.1\", \"0.3…\n$ A_La       <chr> \"11.4\", \"13.8\", \"23.4\", \"20.6\", \"21\", \"16.3\", \"22.3\", \"20.9…\n$ A_Li       <chr> \"5\", \"8\", \"5\", \"21\", \"6\", \"17\", \"9\", \"18\", \"8\", \"4\", \"7\", \"…\n$ A_Mg       <chr> \"0.03\", \"0.05\", \"0.03\", \"0.16\", \"0.06\", \"0.11\", \"0.04\", \"0.…\n$ A_Mn       <chr> \"81\", \"191\", \"249\", \"228\", \"321\", \"96\", \"155\", \"598\", \"76\",…\n$ A_Mo       <chr> \"0.34\", \"0.36\", \"0.41\", \"1.19\", \"0.48\", \"0.66\", \"0.76\", \"0.…\n$ A_Na       <chr> \"<0.01\", \"0.02\", \"<0.01\", \"0.06\", \"0.06\", \"0.05\", \"<0.01\", …\n$ A_Nb       <chr> \"5.2\", \"3.5\", \"3.4\", \"8.4\", \"0.6\", \"9.1\", \"7.1\", \"6.1\", \"2.…\n$ A_Ni       <chr> \"3.2\", \"4.6\", \"4.2\", \"11.5\", \"6.9\", \"8.4\", \"7.7\", \"7\", \"3.8…\n$ A_P        <chr> \"260\", \"170\", \"510\", \"200\", \"200\", \"190\", \"680\", \"440\", \"10…\n$ A_Pb       <chr> \"6.8\", \"11.6\", \"7\", \"14\", \"14.5\", \"15.4\", \"10.5\", \"26.6\", \"…\n$ A_Rb       <chr> \"8.4\", \"20\", \"5.7\", \"57.8\", \"24.6\", \"31.8\", \"9\", \"24.7\", \"1…\n$ A_S        <chr> \"<0.01\", \"<0.01\", \"<0.01\", \"0.01\", \"0.01\", \"0.01\", \"<0.01\",…\n$ A_Sb       <chr> \"0.19\", \"0.23\", \"0.19\", \"0.62\", \"0.1\", \"0.46\", \"0.3\", \"0.4\"…\n$ A_Sc       <chr> \"1.6\", \"1.6\", \"1.4\", \"7.5\", \"2.2\", \"4.1\", \"3.1\", \"2.5\", \"3.…\n$ A_Se       <chr> \"<0.2\", \"<0.2\", \"<0.2\", \"0.5\", \"<0.2\", \"0.4\", \"0.2\", \"0.4\",…\n$ A_Sn       <chr> \"0.5\", \"0.5\", \"0.6\", \"1.3\", \"0.4\", \"1\", \"1.1\", \"1.3\", \"1\", …\n$ A_Sr       <chr> \"6.2\", \"15.2\", \"3.9\", \"30.4\", \"21.9\", \"27.2\", \"6.7\", \"27.9\"…\n$ A_Te       <chr> \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0…\n$ A_Th       <chr> \"4.4\", \"3.2\", \"3\", \"8.2\", \"5\", \"7.1\", \"7.2\", \"5.9\", \"9.2\", …\n$ A_Ti       <chr> \"0.24\", \"0.13\", \"0.13\", \"0.31\", \"0.05\", \"0.28\", \"0.3\", \"0.1…\n$ A_Tl       <chr> \"<0.1\", \"0.2\", \"<0.1\", \"0.5\", \"0.3\", \"0.4\", \"0.1\", \"0.3\", \"…\n$ A_U        <chr> \"1.4\", \"1.1\", \"1\", \"2.4\", \"1.4\", \"2.2\", \"2\", \"1.9\", \"1.9\", …\n$ A_V        <chr> \"14\", \"16\", \"12\", \"56\", \"21\", \"37\", \"27\", \"30\", \"26\", \"22\",…\n$ A_W        <chr> \"0.3\", \"0.3\", \"0.4\", \"0.8\", \"<0.1\", \"0.7\", \"0.6\", \"0.4\", \"<…\n$ A_Y        <chr> \"3.8\", \"4.9\", \"16.3\", \"8.8\", \"8.4\", \"6.7\", \"9.1\", \"9.4\", \"5…\n$ A_Zn       <chr> \"15\", \"17\", \"19\", \"32\", \"32\", \"28\", \"26\", \"48\", \"20\", \"34\",…\n\n\n\n\nCode\nglimpse(meta.geo)\n\n\nRows: 54\nColumns: 3\n$ FIELS_ID    <chr> \"Lab_ID\", \"SiteID\", \"StateID\", \"CollDate\", \"LandCover1\", \"…\n$ Description <chr> \"unique identifier assigned by the analytical laboratories…\n$ Unit        <chr> NA, NA, NA, NA, NA, NA, \"cm\", \"mg/kg\", \"wt. %\", \"mg/kg\", \"…\n\n\nWe first create a file with limited number of variables using select functions and rename them:\n\n\nCode\nmf.geo <- df.geo %>%\n      select(A_LabID, StateID,  LandCover1, A_Depth, A_C_Tot, A_C_Inorg, A_C_Org, A_As, A_Cd,  A_Pb, A_Cr) %>%\n      rename(\"LAB_ID\"=A_LabID,\n             \"LandCover\" =LandCover1,\n             \"Soil_depth\" = A_Depth,\n             \"Total_Carbon\" = A_C_Tot,\n             \"Inorg_Carbon\" = A_C_Inorg,\n             \"Organic_Carbon\"= A_C_Org, \n             \"Arsenic\" = A_As,\n             \"Cadmium\" = A_Cd,\n             \"Lead\" = A_Pb,\n             \"Chromium\" = A_Cr) %>% glimpse()\n\n\nRows: 4,857\nColumns: 11\n$ LAB_ID         <chr> \"C-328943\", \"C-328929\", \"C-328930\", \"C-329034\", \"C-3289…\n$ StateID        <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"…\n$ LandCover      <chr> \"Planted/Cultivated\", \"Developed\", \"Planted/Cultivated\"…\n$ Soil_depth     <chr> \"0-20\", \"0-20\", \"0-20\", \"0-20\", \"0-5\", \"0-10\", \"0-25\", …\n$ Total_Carbon   <chr> \"0.71\", \"0.88\", \"0.65\", \"1.14\", \"1.36\", \"1.71\", \"0.89\",…\n$ Inorg_Carbon   <chr> \"N.D.\", \"N.D.\", \"N.D.\", \"N.D.\", \"N.D.\", \"N.D.\", \"N.D.\",…\n$ Organic_Carbon <chr> \"0.71\", \"0.88\", \"0.65\", \"1.14\", \"1.36\", \"1.71\", \"0.89\",…\n$ Arsenic        <chr> \"1.6\", \"1.5\", \"1.2\", \"3.7\", \"2.7\", \"4.3\", \"2.7\", \"3.7\",…\n$ Cadmium        <chr> \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\", \"<0.1\",…\n$ Lead           <chr> \"6.8\", \"11.6\", \"7\", \"14\", \"14.5\", \"15.4\", \"10.5\", \"26.6…\n$ Chromium       <chr> \"11\", \"14\", \"8\", \"28\", \"17\", \"30\", \"17\", \"22\", \"22\", \"1…\n\n\nNow, we filter the data where records have N.S. values INS:\n\n\nCode\nmf.geo<- mf.geo %>% \n        filter(Soil_depth != \"N.S.\" & Total_Carbon !=\"INS\")\n\n\nThen, we will convert all N.D. values to empty string:\n\n\nCode\nmf.geo[mf.geo==\"N.D.\"]<- \"\"\n\n\nHere detection limits of As, Cd, Pb and Cr are 0.6, 0.1, 0.5, and 1 mg/kg, respectively. We will replace the values below detection limits by half of detection limits of these heavy-metals. Before that we have to remove “<” and convert the all  to .\n\n\nCode\nmf.geo <- mf.geo %>%\n      mutate_at(c(\"Arsenic\", \"Cadmium\", \"Lead\", \"Chromium\"), str_replace, \"<\", \"\") %>%\n      mutate_at(c(5:11), as.numeric) %>% glimpse()\n\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `Inorg_Carbon = .Primitive(\"as.double\")(Inorg_Carbon)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\nRows: 4,809\nColumns: 11\n$ LAB_ID         <chr> \"C-328943\", \"C-328929\", \"C-328930\", \"C-329034\", \"C-3289…\n$ StateID        <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"…\n$ LandCover      <chr> \"Planted/Cultivated\", \"Developed\", \"Planted/Cultivated\"…\n$ Soil_depth     <chr> \"0-20\", \"0-20\", \"0-20\", \"0-20\", \"0-5\", \"0-10\", \"0-25\", …\n$ Total_Carbon   <dbl> 0.71, 0.88, 0.65, 1.14, 1.36, 1.71, 0.89, 4.59, 2.32, 0…\n$ Inorg_Carbon   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Organic_Carbon <dbl> 0.71, 0.88, 0.65, 1.14, 1.36, 1.71, 0.89, 4.59, 2.32, 0…\n$ Arsenic        <dbl> 1.6, 1.5, 1.2, 3.7, 2.7, 4.3, 2.7, 3.7, 2.9, 3.0, 2.3, …\n$ Cadmium        <dbl> 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, …\n$ Lead           <dbl> 6.8, 11.6, 7.0, 14.0, 14.5, 15.4, 10.5, 26.6, 13.2, 12.…\n$ Chromium       <dbl> 11, 14, 8, 28, 17, 30, 17, 22, 22, 10, 17, 10, 31, 11, …\n\n\nNow replace values below detection limits:\n\n\nCode\nmf.geo[\"Arsenic\"][mf.geo[\"Arsenic\"] == 0.6] <- 0.3\nmf.geo[\"Cadmium\"][mf.geo[\"Cadmium\"] == 0.1] <- 0.05\nmf.geo[\"Lead\"][mf.geo[\"Lead\"] == 0.5] <- 0.25\nmf.geo[\"Chromium\"][mf.geo[\"Chromium\"] == 1] <- 0.5\n\n\nNow will check the missing values of the data:\n\n\nCode\n# counting unique, missing, and median values\nArsenic<- mf.geo %>% summarise(N = length(Arsenic),\n                 na = sum(is.na(Arsenic)),\n                 Min = min(Arsenic, na.rm = TRUE),\n                 Max =max(Arsenic, na.rm = TRUE))\n\nCadmium<- mf.geo %>% summarise(N = length(Cadmium),\n                 na = sum(is.na(Cadmium)),\n                 Min = min(Cadmium, na.rm = TRUE),\n                 Max =max(Cadmium, na.rm = TRUE))\n\nLead<- mf.geo %>% summarise(N = length(Lead),\n                 na = sum(is.na(Lead)),\n                 Min = min(Lead, na.rm = TRUE),\n                 Max =max(Lead, na.rm = TRUE))\n\nChromium<- mf.geo %>% summarise(N = length(Chromium),\n                 na = sum(is.na(Chromium)),\n                 Min = min(Chromium, na.rm = TRUE),\n                 Max =max(Chromium, na.rm = TRUE),\n                 )\n\n#bind the data\ngeo.sum<- bind_rows(Arsenic, Cadmium, Lead, Chromium)\n\n#add.row.names\nrow.names(geo.sum) <- c(\"Arsenic\", \"Cadmium\", \"Lead\", \"Chromium\")\n\n\nWarning: Setting row names on a tibble is deprecated.\n\n\nCode\nhead(geo.sum)\n\n\n# A tibble: 4 × 4\n      N    na   Min    Max\n  <int> <int> <dbl>  <dbl>\n1  4809     0  0.3  1110  \n2  4809     0  0.05   46.6\n3  4809     0  0.25 2200  \n4  4809     0  0.5  3850  \n\n\nOne of the common method of replacing missing values is na.omit. The function na.omit() returns the object with listwise deletion of missing values and function create new dataset without missing data.\n\n\nCode\nnewdata <- na.omit(mf.geo)\nglimpse(newdata)\n\n\nRows: 1,167\nColumns: 11\n$ LAB_ID         <chr> \"C-329044\", \"C-329030\", \"C-329081\", \"C-329079\", \"C-3289…\n$ StateID        <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AR\", \"AR\", \"AZ\", \"AZ\", \"…\n$ LandCover      <chr> \"Planted/Cultivated\", \"Developed\", \"Planted/Cultivated\"…\n$ Soil_depth     <chr> \"0-20\", \"0-20\", \"0-15\", \"0-7\", \"0-20\", \"0-10\", \"0-5\", \"…\n$ Total_Carbon   <dbl> 2.90, 4.78, 0.75, 1.69, 1.00, 1.00, 1.88, 0.31, 2.13, 0…\n$ Inorg_Carbon   <dbl> 1.8, 3.7, 0.1, 0.1, 0.2, 0.1, 0.1, 0.2, 0.9, 0.4, 0.1, …\n$ Organic_Carbon <dbl> 1.1, 1.1, 0.7, 1.6, 0.8, 0.9, 1.8, 0.1, 1.2, 0.3, 0.8, …\n$ Arsenic        <dbl> 5.9, 7.9, 5.6, 7.2, 2.9, 4.8, 1.9, 5.3, 4.5, 5.0, 4.0, …\n$ Cadmium        <dbl> 0.20, 0.20, 0.05, 0.20, 0.05, 0.05, 0.05, 0.20, 0.20, 0…\n$ Lead           <dbl> 20.9, 19.1, 12.9, 21.1, 14.1, 11.9, 8.7, 26.8, 14.7, 24…\n$ Chromium       <dbl> 73, 68, 37, 42, 36, 31, 16, 36, 35, 28, 30, 25, 62, 21,…\n\n\nThis function delete all records with missing values. Now newdata have only 1,167 records, since Inorg_Carbon variable has 3,690 missing values which all are omitted.\nWe can impute missing values in several ways. The easiest way to impute missing values is by replacing the mean values of the variable. The following code replace missing of Arsenic, Cadmium, Lead and Chromium with their mean values.\n\n\nCode\nmf.geo.new<-mf.geo %>% mutate_at(vars(Arsenic, Cadmium, Lead, Chromium),~ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)) %>% glimpse()\n\n\nRows: 4,809\nColumns: 11\n$ LAB_ID         <chr> \"C-328943\", \"C-328929\", \"C-328930\", \"C-329034\", \"C-3289…\n$ StateID        <chr> \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"…\n$ LandCover      <chr> \"Planted/Cultivated\", \"Developed\", \"Planted/Cultivated\"…\n$ Soil_depth     <chr> \"0-20\", \"0-20\", \"0-20\", \"0-20\", \"0-5\", \"0-10\", \"0-25\", …\n$ Total_Carbon   <dbl> 0.71, 0.88, 0.65, 1.14, 1.36, 1.71, 0.89, 4.59, 2.32, 0…\n$ Inorg_Carbon   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Organic_Carbon <dbl> 0.71, 0.88, 0.65, 1.14, 1.36, 1.71, 0.89, 4.59, 2.32, 0…\n$ Arsenic        <dbl> 1.6, 1.5, 1.2, 3.7, 2.7, 4.3, 2.7, 3.7, 2.9, 3.0, 2.3, …\n$ Cadmium        <dbl> 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.20, 0.05, 0…\n$ Lead           <dbl> 6.8, 11.6, 7.0, 14.0, 14.5, 15.4, 10.5, 26.6, 13.2, 12.…\n$ Chromium       <dbl> 11, 14, 8, 28, 17, 30, 17, 22, 22, 10, 17, 10, 31, 11, …\n\n\n\n\nCode\nmf.geo.new %>%summarise(sum(is.na(Chromium)))\n\n\n# A tibble: 1 × 1\n  `sum(is.na(Chromium))`\n                   <int>\n1                      0\n\n\nCode\n# save clean data\n#write_csv(mf.geo.new,\"usa_geochemical_clean.csv\" )\n\n\n\n\nExercise\n\nCreate a folder in your computer name Homework.\nCreate a R-Markdown Project on this Homework directory\nCreate a R-Markdown documents (name homework_01.rmd) in this project directory and do all Tasks (1 to 23) using the data shown below.\nSubmit all codes and output as a HTML document (homework_02.html) before class of next week.\n\n\nRequired R-Package\n\nlibrary(tidyverse)\n\n\n\nData\n\nbd_rice_production.csv: Yearly rice production of Bangladesh\nbd_district.csv: District names with DISTRICT_ID and DIVISION_PCODE\nbd_division.csv Division names and DIVISION_PCODE\nraj_soil_data.csv: SRDI soil data for Rajshahi Division\nbd_soil_data_raw.csv: SRDI soil data, not cleaned\n\nDownload all above data and save in your project directory. Use read_csv to load the data in your R-session. For example:\n\nrice<-read_csv(“bd_rice_production.csv”))\n\n\n\nTasks\n\nJoin rice production, and district file using dplyr::inner_join()\nThen join division file using dplyr::inner_join()\nOrganize column as (DIVISION_ID, DIVISION_NAME, DISTRICT_ID, DISTRICT_NAME, YEAR, MT using dplyr::relocate()\nRename “DISTRICT_ID” to “DISTRICT_PCODE” using dplyr::rename()\nRun all the above code using pipe function in a single line and create a data frame name df.rice.csv\nForm df.rice data that you have created before, use select() function to create a data frame only with DISTRICT_NAME, YEAR and MT\nFilter out the data from Rajshahi division using dplyr::filter()\nFilter (Filtering by multiple criteria) all district in Rajshahi and Dhaka divisions using dplyr::filter()\nUse | or OR in the logical condition in filter function, select data from any of the two divisions.\nApply filter create a files for the Rajshai Division only with Bogra District.\nSelect districts where rice production (MT) is greater than the global average\nUse & to select district where MT is greater than the global average of for the year 2018-2019\nSelect districts starting with “P”\nUse raj_soil data to calculate mean of SOM using dplyr::summarise_at() function\nUse bd_soil data to calculate mean of SOM and NDVI using dplyr::summarise_at() function\nUse dplyr::summarise_if() to calculate mean() to the numeric columns\nSelect numerical columns (pH,SOM, DEM, NDVI, NDFI) and then apply dplyr::summarise_all()*to calculate mean of these variables\nCreate a new column (MT_1000) in df.rice dataframe dividing MT column by 1000 using mutate()\nApply the group_by(), summarize()** and mutate() functions with pipe to calculate mean of corn production in 1000 MT by division for the year 2018-2019\nUse df.rice data to create a wider data frame using tidyr::pivot_wider() function\nConvert rice.wider data to a longer format use tidyr::pivot_longer()\nClean bd_soil_data_raw.csv and save as bd_soil_data.final.csv. Follow the steps you have learned from “Cleaning a messy data” section.\n\n\n\n\nFurther Reading\n\nR for Data Science\nData Wrangling with R\nBook: Data Wrangling with R\nPDF Data wrangling with R and RStudio\nYoutube Data Wrangling with R and the Tidyverse - Workshop"
  },
  {
    "objectID": "data-wrgaling-janitor.html",
    "href": "data-wrgaling-janitor.html",
    "title": "Data Science with R For Envrionmental Modeling",
    "section": "",
    "text": "Data Wrangling with Janitor\n\nThe R package janitor provides a set of tools for cleaning and organizing data in R. The package is designed to help make data cleaning tasks easier and more efficient, with functions that handle common data cleaning tasks.\nSome of the functions provided by the janitor package include:\nclean_names: This function cleans column names by removing special characters and converting them to lowercase.\nremove_empty: This function removes rows or columns that are entirely empty from a data frame.\ntabyl: This function creates frequency tables with ease.\nget_dupes: This function identifies duplicate rows in a data frame.\nfactorize: This function converts columns in a data frame to factors.\nThe janitor package can be installed using the following command in R:\n\ninstall.packages(“janitor”)\n\n\nLoad Packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\n\nSome Important functions\nWe will create some “bad” data and clean them with janitor. We will apply following functions:\n\nclean_names()\nremove_empty()\ntrim_ws()\nget_dupes()\nremove_constant()\n\n\nclean_names()\nThe clean_names() function is used to clean column names in a data frame. It converts the column names to lowercase and replaces all spaces and special characters with underscores.\n\n\nCode\n# Create a data frame with messy column names\ndf <- data.frame(\"Column One\" = 1:5, \n                 \"Column Two!!\" = 6:10, \n                 \"Column Three $\" = 11:15,\n                 \"%Column four\" = 11:15)\nhead(df)\n\n\n  Column.One Column.Two.. Column.Three.. X.Column.four\n1          1            6             11            11\n2          2            7             12            12\n3          3            8             13            13\n4          4            9             14            14\n5          5           10             15            15\n\n\n\n\nCode\ndf %>%\n  janitor::clean_names()\n\n\n  column_one column_two column_three x_column_four\n1          1          6           11            11\n2          2          7           12            12\n3          3          8           13            13\n4          4          9           14            14\n5          5         10           15            15\n\n\n\n\nremove_empty()\nThe remove_empty() function is used to remove rows or columns that contain only missing or empty values.\n\n\nCode\n# Create a data frame with empty rows and columns\ndf <-  data.frame(x = c(1,NA,4),\n                    y = c(NA,NA,3),\n                    z = c(NA, NA, NA))\n\nhead(df)\n\n\n   x  y  z\n1  1 NA NA\n2 NA NA NA\n3  4  3 NA\n\n\n\n\nCode\ndf %>% \n  janitor::remove_empty(c(\"rows\",\"cols\"))\n\n\n  x  y\n1 1 NA\n3 4  3\n\n\n\n\nget_dupes()\nThe get_dupes() function is used to find duplicate rows in a data frame.\n\n\nCode\ndf <- data.frame(\"Column One\" = c(1, 2, 3, 1), \"Column Two\" = c(\"A\", \"B\", \"C\", \"A\"))\nget_dupes(df)\n\n\n  Column.One Column.Two dupe_count\n1          1          A          2\n2          1          A          2\n\n\n\n\nClean a bad data\nNow we will clean on very messy data using some functions of janitor packages. We will use Lung Cancer Mortality data.\n\n\nCode\n## Create a data folder\ndataFolder<-\"E:/Dropbox/GitHub/Data/USA/\"\ndf<-read_csv(paste0(dataFolder,\"USA_LBC_Data.csv\"))\n\n\n\n\nCode\nglimpse(df)\n\n\nRows: 3,118\nColumns: 26\n$ `Lung Cancer Moratlity Rates and Risk in USA, Data Provider: Zia Ahmed` <chr> …\n$ ...2                                                                    <chr> …\n$ ...3                                                                    <chr> …\n$ ...4                                                                    <chr> …\n$ ...5                                                                    <chr> …\n$ ...6                                                                    <chr> …\n$ ...7                                                                    <chr> …\n$ ...8                                                                    <chr> …\n$ ...9                                                                    <chr> …\n$ ...10                                                                   <chr> …\n$ ...11                                                                   <chr> …\n$ ...12                                                                   <chr> …\n$ ...13                                                                   <chr> …\n$ ...14                                                                   <chr> …\n$ ...15                                                                   <chr> …\n$ ...16                                                                   <chr> …\n$ ...17                                                                   <chr> …\n$ ...18                                                                   <chr> …\n$ ...19                                                                   <chr> …\n$ ...20                                                                   <chr> …\n$ ...21                                                                   <chr> …\n$ ...22                                                                   <chr> …\n$ ...23                                                                   <chr> …\n$ ...24                                                                   <chr> …\n$ ...25                                                                   <chr> …\n$ ...26                                                                   <chr> …\n\n\nYou’ve probably received plenty of data files like this, that have some text at the top of the spreadsheet before the actual data begins.\nIn this df.lbc data-frame, the column heading describe briefly the data. But we want 1st row as column heading. So we apply we will apply row_to_names(). The row_to_names() function takes the following arguments: the data source, the row number that column names should come from, whether that row should be deleted from the data, and whether the rows above should be deleted from the data:\n\n\nCode\ndf.01 = df %>% \n  janitor::row_to_names(1, remove_row = TRUE, remove_rows_above = TRUE) %>%\n  glimpse()\n\n\nRows: 3,117\nColumns: 26\n$ REGION_ID            <chr> \"3\", \"3\", \"3\", \"3\", NA, NA, \"3\", \"3\", \"3\", \"3\", \"…\n$ STATE                <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", NA, N…\n$ County               <chr> \"Baldwin County\", \"Butler County\", \"Butler County…\n$ `Empty Column 1`     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ X                    <chr> \"789777.5039\", \"877731.5725\", \"877731.5725\", \"984…\n$ Y                    <chr> \"884557.0795\", \"1007285.71\", \"1007285.71\", \"11486…\n$ Fips                 <chr> \"1003\", \"1013\", \"1013\", \"1017\", NA, NA, \"1023\", \"…\n$ `Empty_Column 2`     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `LCB Mortality Rate` <chr> \"48.1\", \"38.3\", \"38.3\", \"49.6\", NA, NA, \"31.8\", \"…\n$ Smoking              <chr> \"20.8\", \"26\", \"26\", \"25.1\", NA, NA, \"21.8\", \"22.6…\n$ `PM  25`             <chr> \"7.89\", \"8.46\", \"8.46\", \"8.87\", NA, NA, \"8.58\", \"…\n$ NO2                  <chr> \"0.7939\", \"0.6344\", \"0.6344\", \"0.8442\", NA, NA, \"…\n$ SO2                  <chr> \"0.035343\", \"0.0135\", \"0.0135\", \"0.048177\", NA, N…\n$ Ozone                <chr> \"39.79\", \"38.31\", \"38.31\", \"40.1\", NA, NA, \"37.07…\n$ `Pop 65`             <chr> \"19.5\", \"19\", \"19\", \"18.9\", NA, NA, \"22.1\", \"19\",…\n$ `Pop Black`          <chr> \"9.24\", \"43.94\", \"43.94\", \"39.24\", NA, NA, \"41.94…\n$ `Pop Hipanic`        <chr> \"4.54\", \"1.26\", \"1.26\", \"2.14\", NA, NA, \"0.86\", \"…\n$ `Pop White`          <chr> \"83.06\", \"52.64\", \"52.64\", \"56.42\", NA, NA, \"56.2…\n$ Education            <chr> \"66\", \"38\", \"38\", \"47\", NA, NA, \"55\", \"39\", \"60\",…\n$ `Poverty %`          <chr> \"13.14\", \"26.14\", \"26.14\", \"21.52\", NA, NA, \"23.0…\n$ `Income Equality`    <chr> \"4.5\", \"5.1\", \"5.1\", \"4.7\", NA, NA, \"5.8\", \"8.2\",…\n$ Uninsured            <chr> \"13.34\", \"12.74\", \"12.74\", \"13.34\", NA, NA, \"12.8…\n$ DEM                  <chr> \"36.78\", \"111.70\", \"111.70\", \"227.03\", NA, NA, \"6…\n$ `Radon Zone Class`   <chr> \"Zone-3\", \"Zone-3\", \"Zone-3\", \"Zone-3\", NA, NA, \"…\n$ `Urban Rural`        <chr> \"Medium/small metro\", \"Nonmetro\", \"Nonmetro\", \"No…\n$ `Coal Production`    <chr> \"No\", \"No\", \"No\", \"No\", NA, NA, \"No\", \"No\", \"No\",…\n\n\nStill data has some empty columns and and empty rows, we are going to remove these empty columns and rows using remove_empty() function:\n\n\nCode\ndf.02 = df.01 %>% \n  janitor::remove_empty() %>%\n  glimpse()\n\n\nRows: 3,110\nColumns: 24\n$ REGION_ID            <chr> \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\",…\n$ STATE                <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alab…\n$ County               <chr> \"Baldwin County\", \"Butler County\", \"Butler County…\n$ X                    <chr> \"789777.5039\", \"877731.5725\", \"877731.5725\", \"984…\n$ Y                    <chr> \"884557.0795\", \"1007285.71\", \"1007285.71\", \"11486…\n$ Fips                 <chr> \"1003\", \"1013\", \"1013\", \"1017\", \"1023\", \"1025\", \"…\n$ `LCB Mortality Rate` <chr> \"48.1\", \"38.3\", \"38.3\", \"49.6\", \"31.8\", \"42\", \"53…\n$ Smoking              <chr> \"20.8\", \"26\", \"26\", \"25.1\", \"21.8\", \"22.6\", \"21.2…\n$ `PM  25`             <chr> \"7.89\", \"8.46\", \"8.46\", \"8.87\", \"8.58\", \"8.42\", \"…\n$ NO2                  <chr> \"0.7939\", \"0.6344\", \"0.6344\", \"0.8442\", \"0.5934\",…\n$ SO2                  <chr> \"0.035343\", \"0.0135\", \"0.0135\", \"0.048177\", \"0.02…\n$ Ozone                <chr> \"39.79\", \"38.31\", \"38.31\", \"40.1\", \"37.07\", \"37.6…\n$ `Pop 65`             <chr> \"19.5\", \"19\", \"19\", \"18.9\", \"22.1\", \"19\", \"16.3\",…\n$ `Pop Black`          <chr> \"9.24\", \"43.94\", \"43.94\", \"39.24\", \"41.94\", \"43.9…\n$ `Pop Hipanic`        <chr> \"4.54\", \"1.26\", \"1.26\", \"2.14\", \"0.86\", \"1.34\", \"…\n$ `Pop White`          <chr> \"83.06\", \"52.64\", \"52.64\", \"56.42\", \"56.28\", \"52.…\n$ Education            <chr> \"66\", \"38\", \"38\", \"47\", \"55\", \"39\", \"60\", \"35\", \"…\n$ `Poverty %`          <chr> \"13.14\", \"26.14\", \"26.14\", \"21.52\", \"23.06\", \"24.…\n$ `Income Equality`    <chr> \"4.5\", \"5.1\", \"5.1\", \"4.7\", \"5.8\", \"8.2\", \"4.8\", …\n$ Uninsured            <chr> \"13.34\", \"12.74\", \"12.74\", \"13.34\", \"12.86\", \"13.…\n$ DEM                  <chr> \"36.78\", \"111.70\", \"111.70\", \"227.03\", \"68.24\", \"…\n$ `Radon Zone Class`   <chr> \"Zone-3\", \"Zone-3\", \"Zone-3\", \"Zone-3\", \"Zone-3\",…\n$ `Urban Rural`        <chr> \"Medium/small metro\", \"Nonmetro\", \"Nonmetro\", \"No…\n$ `Coal Production`    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"…\n\n\nNow, we are going fix column headings using clean_names(). It converts the column names to lowercase and replaces all spaces and special characters with underscores.\n\n\nCode\ndf.03 = df.02 %>%\n  janitor::clean_names() %>%\n  glimpse()\n\n\nRows: 3,110\nColumns: 24\n$ region_id          <chr> \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"…\n$ state              <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabam…\n$ county             <chr> \"Baldwin County\", \"Butler County\", \"Butler County\",…\n$ x                  <chr> \"789777.5039\", \"877731.5725\", \"877731.5725\", \"98421…\n$ y                  <chr> \"884557.0795\", \"1007285.71\", \"1007285.71\", \"1148648…\n$ fips               <chr> \"1003\", \"1013\", \"1013\", \"1017\", \"1023\", \"1025\", \"10…\n$ lcb_mortality_rate <chr> \"48.1\", \"38.3\", \"38.3\", \"49.6\", \"31.8\", \"42\", \"53.7…\n$ smoking            <chr> \"20.8\", \"26\", \"26\", \"25.1\", \"21.8\", \"22.6\", \"21.2\",…\n$ pm_25              <chr> \"7.89\", \"8.46\", \"8.46\", \"8.87\", \"8.58\", \"8.42\", \"8.…\n$ no2                <chr> \"0.7939\", \"0.6344\", \"0.6344\", \"0.8442\", \"0.5934\", \"…\n$ so2                <chr> \"0.035343\", \"0.0135\", \"0.0135\", \"0.048177\", \"0.0239…\n$ ozone              <chr> \"39.79\", \"38.31\", \"38.31\", \"40.1\", \"37.07\", \"37.68\"…\n$ pop_65             <chr> \"19.5\", \"19\", \"19\", \"18.9\", \"22.1\", \"19\", \"16.3\", \"…\n$ pop_black          <chr> \"9.24\", \"43.94\", \"43.94\", \"39.24\", \"41.94\", \"43.96\"…\n$ pop_hipanic        <chr> \"4.54\", \"1.26\", \"1.26\", \"2.14\", \"0.86\", \"1.34\", \"6.…\n$ pop_white          <chr> \"83.06\", \"52.64\", \"52.64\", \"56.42\", \"56.28\", \"52.98…\n$ education          <chr> \"66\", \"38\", \"38\", \"47\", \"55\", \"39\", \"60\", \"35\", \"53…\n$ poverty_percent    <chr> \"13.14\", \"26.14\", \"26.14\", \"21.52\", \"23.06\", \"24.6\"…\n$ income_equality    <chr> \"4.5\", \"5.1\", \"5.1\", \"4.7\", \"5.8\", \"8.2\", \"4.8\", \"4…\n$ uninsured          <chr> \"13.34\", \"12.74\", \"12.74\", \"13.34\", \"12.86\", \"13.28…\n$ dem                <chr> \"36.78\", \"111.70\", \"111.70\", \"227.03\", \"68.24\", \"69…\n$ radon_zone_class   <chr> \"Zone-3\", \"Zone-3\", \"Zone-3\", \"Zone-3\", \"Zone-3\", \"…\n$ urban_rural        <chr> \"Medium/small metro\", \"Nonmetro\", \"Nonmetro\", \"Nonm…\n$ coal_production    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n\n\nAll data are exported in R as chr. We are going to convert column from 4 to 21 as.numeric and 22 to 23 as.factor. We use dplyr::mutate_at() function:\n\n\nCode\ndf.04= df.03 %>%\n     dplyr::mutate_at(4:21, as.numeric) %>%\n     dplyr::mutate_at(22:24, as.factor) %>%\n     glimpse()\n\n\nRows: 3,110\nColumns: 24\n$ region_id          <chr> \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"…\n$ state              <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabam…\n$ county             <chr> \"Baldwin County\", \"Butler County\", \"Butler County\",…\n$ x                  <dbl> 789777.5, 877731.6, 877731.6, 984214.7, 726606.5, 7…\n$ y                  <dbl> 884557.1, 1007285.7, 1007285.7, 1148648.7, 1023615.…\n$ fips               <dbl> 1003, 1013, 1013, 1017, 1023, 1025, 1031, 1035, 103…\n$ lcb_mortality_rate <dbl> 48.1, 38.3, 38.3, 49.6, 31.8, 42.0, 53.7, 46.9, 65.…\n$ smoking            <dbl> 20.8, 26.0, 26.0, 25.1, 21.8, 22.6, 21.2, 24.9, 25.…\n$ pm_25              <dbl> 7.89, 8.46, 8.46, 8.87, 8.58, 8.42, 8.42, 8.23, 8.2…\n$ no2                <dbl> 0.7939, 0.6344, 0.6344, 0.8442, 0.5934, 0.6432, 0.5…\n$ so2                <dbl> 0.035343, 0.013500, 0.013500, 0.048177, 0.023989, 0…\n$ ozone              <dbl> 39.79, 38.31, 38.31, 40.10, 37.07, 37.68, 38.46, 37…\n$ pop_65             <dbl> 19.5, 19.0, 19.0, 18.9, 22.1, 19.0, 16.3, 21.6, 20.…\n$ pop_black          <dbl> 9.24, 43.94, 43.94, 39.24, 41.94, 43.96, 17.26, 45.…\n$ pop_hipanic        <dbl> 4.54, 1.26, 1.26, 2.14, 0.86, 1.34, 6.76, 1.84, 1.6…\n$ pop_white          <dbl> 83.06, 52.64, 52.64, 56.42, 56.28, 52.98, 70.90, 50…\n$ education          <dbl> 66, 38, 38, 47, 55, 39, 60, 35, 53, 44, 58, 38, 38,…\n$ poverty_percent    <dbl> 13.14, 26.14, 26.14, 21.52, 23.06, 24.60, 16.20, 29…\n$ income_equality    <dbl> 4.5, 5.1, 5.1, 4.7, 5.8, 8.2, 4.8, 4.9, 4.6, 5.8, 5…\n$ uninsured          <dbl> 13.34, 12.74, 12.74, 13.34, 12.86, 13.28, 13.16, 15…\n$ dem                <dbl> 36.78, 111.70, 111.70, 227.03, 68.24, 69.29, 99.32,…\n$ radon_zone_class   <fct> Zone-3, Zone-3, Zone-3, Zone-3, Zone-3, Zone-3, Zon…\n$ urban_rural        <fct> Medium/small metro, Nonmetro, Nonmetro, Nonmetro, N…\n$ coal_production    <fct> No, No, No, No, No, No, No, No, No, No, No, No, No,…\n\n\nNow will check the duplicates record in the this data:\n\n\nCode\ndf.04 %>% janitor::get_dupes(fips)\n\n\n# A tibble: 6 × 25\n   fips dupe_co…¹ regio…² state county      x      y lcb_m…³ smoking pm_25   no2\n  <dbl>     <int> <chr>   <chr> <chr>   <dbl>  <dbl>   <dbl>   <dbl> <dbl> <dbl>\n1  1013         2 3       Alab… Butle… 8.78e5 1.01e6    38.3    26    8.46 0.634\n2  1013         2 3       Alab… Butle… 8.78e5 1.01e6    38.3    26    8.46 0.634\n3  1053         2 3       Alab… Escam… 8.39e5 9.34e5    58.3    25.3  8.08 0.574\n4  1053         2 3       Alab… Escam… 8.39e5 9.34e5    58.3    25.3  8.08 0.574\n5  5011         2 3       Arka… Bradl… 3.54e5 1.16e6    69.9    25    8.32 0.558\n6  5011         2 3       Arka… Bradl… 3.54e5 1.16e6    69.9    25    8.32 0.558\n# … with 14 more variables: so2 <dbl>, ozone <dbl>, pop_65 <dbl>,\n#   pop_black <dbl>, pop_hipanic <dbl>, pop_white <dbl>, education <dbl>,\n#   poverty_percent <dbl>, income_equality <dbl>, uninsured <dbl>, dem <dbl>,\n#   radon_zone_class <fct>, urban_rural <fct>, coal_production <fct>, and\n#   abbreviated variable names ¹​dupe_count, ²​region_id, ³​lcb_mortality_rate\n\n\nAs shown above, the data frame is filtered down to those rows with duplicate values in the Fips column. For removing these duplicate rows, we have to use dplyr::distinct(.keep_all = TRUE)\nNow will check the duplicates record in the this data:\n\n\nCode\ndf.05= df.04 %>% \n     dplyr::distinct(fips,.keep_all = TRUE) %>%\n     glimpse()\n\n\nRows: 3,107\nColumns: 24\n$ region_id          <chr> \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"…\n$ state              <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabam…\n$ county             <chr> \"Baldwin County\", \"Butler County\", \"Chambers County…\n$ x                  <dbl> 789777.5, 877731.6, 984214.7, 726606.5, 770408.9, 9…\n$ y                  <dbl> 884557.1, 1007285.7, 1148648.7, 1023615.8, 988910.5…\n$ fips               <dbl> 1003, 1013, 1017, 1023, 1025, 1031, 1035, 1039, 104…\n$ lcb_mortality_rate <dbl> 48.1, 38.3, 49.6, 31.8, 42.0, 53.7, 46.9, 65.5, 57.…\n$ smoking            <dbl> 20.8, 26.0, 25.1, 21.8, 22.6, 21.2, 24.9, 25.9, 22.…\n$ pm_25              <dbl> 7.89, 8.46, 8.87, 8.58, 8.42, 8.42, 8.23, 8.24, 8.4…\n$ no2                <dbl> 0.7939, 0.6344, 0.8442, 0.5934, 0.6432, 0.5698, 0.5…\n$ so2                <dbl> 0.035343, 0.013500, 0.048177, 0.023989, 0.033700, 0…\n$ ozone              <dbl> 39.79, 38.31, 40.10, 37.07, 37.68, 38.46, 37.92, 38…\n$ pop_65             <dbl> 19.5, 19.0, 18.9, 22.1, 19.0, 16.3, 21.6, 20.5, 18.…\n$ pop_black          <dbl> 9.24, 43.94, 39.24, 41.94, 43.96, 17.26, 45.94, 12.…\n$ pop_hipanic        <dbl> 4.54, 1.26, 2.14, 0.86, 1.34, 6.76, 1.84, 1.62, 1.8…\n$ pop_white          <dbl> 83.06, 52.64, 56.42, 56.28, 52.98, 70.90, 50.56, 83…\n$ education          <dbl> 66, 38, 47, 55, 39, 60, 35, 53, 44, 58, 38, 45, 58,…\n$ poverty_percent    <dbl> 13.14, 26.14, 21.52, 23.06, 24.60, 16.20, 29.76, 20…\n$ income_equality    <dbl> 4.5, 5.1, 4.7, 5.8, 8.2, 4.8, 4.9, 4.6, 5.8, 5.2, 5…\n$ uninsured          <dbl> 13.34, 12.74, 13.34, 12.86, 13.28, 13.16, 15.16, 13…\n$ dem                <dbl> 36.78, 111.70, 227.03, 68.24, 69.29, 99.32, 96.03, …\n$ radon_zone_class   <fct> Zone-3, Zone-3, Zone-3, Zone-3, Zone-3, Zone-3, Zon…\n$ urban_rural        <fct> Medium/small metro, Nonmetro, Nonmetro, Nonmetro, N…\n$ coal_production    <fct> No, No, No, No, No, No, No, No, No, No, No, No, No,…\n\n\nNow we run all above function with Pipe (%>%):\n\n\nCode\ndf_clean = df %>% \n  janitor::row_to_names(1, remove_row = TRUE, remove_rows_above = TRUE) %>%\n  janitor::remove_empty() %>%\n  janitor::clean_names() %>%\n  dplyr::mutate_at(4:21, as.numeric) %>%\n  dplyr::mutate_at(22:24, as.factor) %>%\n  dplyr::distinct(fips,.keep_all = TRUE) %>%\n     glimpse()\n\n\nRows: 3,107\nColumns: 24\n$ region_id          <chr> \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"…\n$ state              <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabam…\n$ county             <chr> \"Baldwin County\", \"Butler County\", \"Chambers County…\n$ x                  <dbl> 789777.5, 877731.6, 984214.7, 726606.5, 770408.9, 9…\n$ y                  <dbl> 884557.1, 1007285.7, 1148648.7, 1023615.8, 988910.5…\n$ fips               <dbl> 1003, 1013, 1017, 1023, 1025, 1031, 1035, 1039, 104…\n$ lcb_mortality_rate <dbl> 48.1, 38.3, 49.6, 31.8, 42.0, 53.7, 46.9, 65.5, 57.…\n$ smoking            <dbl> 20.8, 26.0, 25.1, 21.8, 22.6, 21.2, 24.9, 25.9, 22.…\n$ pm_25              <dbl> 7.89, 8.46, 8.87, 8.58, 8.42, 8.42, 8.23, 8.24, 8.4…\n$ no2                <dbl> 0.7939, 0.6344, 0.8442, 0.5934, 0.6432, 0.5698, 0.5…\n$ so2                <dbl> 0.035343, 0.013500, 0.048177, 0.023989, 0.033700, 0…\n$ ozone              <dbl> 39.79, 38.31, 40.10, 37.07, 37.68, 38.46, 37.92, 38…\n$ pop_65             <dbl> 19.5, 19.0, 18.9, 22.1, 19.0, 16.3, 21.6, 20.5, 18.…\n$ pop_black          <dbl> 9.24, 43.94, 39.24, 41.94, 43.96, 17.26, 45.94, 12.…\n$ pop_hipanic        <dbl> 4.54, 1.26, 2.14, 0.86, 1.34, 6.76, 1.84, 1.62, 1.8…\n$ pop_white          <dbl> 83.06, 52.64, 56.42, 56.28, 52.98, 70.90, 50.56, 83…\n$ education          <dbl> 66, 38, 47, 55, 39, 60, 35, 53, 44, 58, 38, 45, 58,…\n$ poverty_percent    <dbl> 13.14, 26.14, 21.52, 23.06, 24.60, 16.20, 29.76, 20…\n$ income_equality    <dbl> 4.5, 5.1, 4.7, 5.8, 8.2, 4.8, 4.9, 4.6, 5.8, 5.2, 5…\n$ uninsured          <dbl> 13.34, 12.74, 13.34, 12.86, 13.28, 13.16, 15.16, 13…\n$ dem                <dbl> 36.78, 111.70, 227.03, 68.24, 69.29, 99.32, 96.03, …\n$ radon_zone_class   <fct> Zone-3, Zone-3, Zone-3, Zone-3, Zone-3, Zone-3, Zon…\n$ urban_rural        <fct> Medium/small metro, Nonmetro, Nonmetro, Nonmetro, N…\n$ coal_production    <fct> No, No, No, No, No, No, No, No, No, No, No, No, No,…\n\n\n\n\n\nExercise\n\nCreate a R-Markdown Project on exiting Homework directory\nCreate a R-Markdown documents (name homework_02.rmd) in this project directory and do all Tasks (1 to 6) using the data shown below.\nSubmit all codes and output as a HTML document (homework_02.html) before class of next week.\n\n\nRequired R-Package\ntidyverse and janitor\n\n\nData\nbd_arsenic_data_raw.csv\nDownload the data and save in your project directory. Use read_csv to load the data in your R-session. For example:\n\nbf<-read_csv(“bd_arsenic_data_raw.csv”))\n\n\n\nTasks\n\nUse janitor::row_to_names() remove text from column heading\nRemove empty rows and columns using janitor::remove_empty()\nClean column names using janitor::clean_names()\nUse as.numeric and as.factor arguments in dplyr::mutate_at() function to convert ‘chr’ columns to numeric and factors accordingly\nFind duplicate records and remove them\nRun all above functions with pipe.\n\n\n\n\nFurther Reading\n\nOverview of janitor functions\nCleaning and Exploring Data with the “janitor” Package"
  },
  {
    "objectID": "download-install-r.html",
    "href": "download-install-r.html",
    "title": "Data Science with R For Envrionmental Modeling",
    "section": "",
    "text": "This tutorial will teach you how to begin programming with R using RStudio. We’ll install R, and RStudio, a popular development environment for R. We’ll also learn how to install R-package and some key RStudio features to start programming in R on our own.\n\n\nYou can download the latest version of R from the Comprehensive R Archive Network (CRAN) website (https://cran.r-project.org/). Follow the instructions to install it on your computer. The version of R to download depends on our operating system. We can also install Microsoft R Open, the enhanced distribution of R from Microsoft. It includes additional capabilities for improved performance and reproducibility and support for Windows and Linux-based platforms.\n\nClick this link to download the latest stable version of R\nSelect a CRAN location (a mirror site) close to you\nClick on the “Download R for Windows”\nClick on the “install R for the first time” link at the top corner of the page\nClick “Download R for Windows” and it will download to local download folder\n\n\n\n\n\nInstallation instruction of R in Windows and MAC could be found here. Detail Installation steps of Microsoft R Open in different operating systems can be found here.\n\n\n\nOpen Windows Explorer\nNavigate to the directory where the downloaded R.4.2.#-win.exe (the latest version) file is exit.\nDouble-click this exe file and follow the instruction as shown in the video below:\n\n\n\n\n\nInstalling R on Mac OS is similar to Windows. The easiest way is to install it through CRAN by going to the CRAN downloads page and following description as as shown here. In brief, download the .pkg and open the .pkg file and follow the standard instructions for installing applications on MAC OS X.\n\n\n\nStep 1: Update and Upgrade the Ubuntu 22.04\nTo update and upgrade packages on Ubuntu 22.04, firstly, open up the terminal by pressing “CTRL+ALT+T” and execute the following command:\n\nsudo apt update\n\n\nsudo apt upgrade\n\nStep 2: Add dependencies\nNow, on your Ubuntu 22.04 system, download and install required dependencies. These dependencies are used to execute the R on Ubuntu 22.04. Run the following command for the specified purpose:\n\nsudo apt install dirmngr gnupg apt-transport-https ca-certificates software-properties-common\n\nStep 3: Authenticate the packages\nAuthenticate the installed packages by writing out the following command:\n\nsudo apt-key adv –keyserver keyserver.ubuntu.com –recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n\nStep 4: Import GPG key\nNext, import the GPG key in your system through below command:\n\nwget -O- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo gpg –dearmor | sudo tee /usr/share/keyrings/cran.gpg\n\nStep 5: Add CRAN repository\nAfter importing the GPG key, you need to add the CRAN repository that is highly recommended for R installation:\n\necho deb [signed-by=/usr/share/keyrings/cran.gpg] https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/ | sudo tee /etc/apt/sources.list.d/cran.list\n\n\nsudo apt update\n\nStep 6: Install R through CRAN repository Now, your Ubuntu 22.04 system is ready to install the R language from the CRAN repository. To do so, type the following command:\n\nsudo apt install r-base\n\n\nsudo apt-get install libcurl4-openssl-dev libssl-dev libxml2-dev liblapack-dev\n\nStep 7: Check R -version\n\nR –version\n\n\n\n\nAfter R installation in Windows, double click on the desktop icon or open the program from START to run R. R will be open as a Console window.\n\n\nFigure 1: Interface of R-base\n\n\n\nYou can work in the console and the command line. However, the command line can be pretty daunting to a beginner. It is better to work in R Editor. First, you must create a New script from File menu. Any code you run in R-script output will be displayed in the console window. You can save all your R codes as an R script file and output in the console as an R-Data file.\n\n\n\n\n\nRtools is a collection of software tools that are used to build packages for the R programming language on Windows operating system. It includes various programs and libraries, such as make, gcc, g++, and others, that are needed to compile and build R packages that contain compiled code. Without Rtools, it is not possible to build such packages on Windows.\nRtools is not needed if you only use R for data analysis and use packages that only contain interpreted code. However, if you need to use a package that contains compiled code, you will need to install Rtools in order to use that package.\nThe version of may be installed from the Rtools installer. It is recommended to use the defaults, including the default installation location of C:.\nWhen using R installed by the installer, no further setup is necessary after installing Rtools to build R packages from source. When using the default installation location, R and Rtools42 may be installed in any order and Rtools42 may be installed when R is already running.\n\n\n\nR can be run in the command line and graphical user interfaces in integrated development environment (IDE). Below are the best programming IDE for R:\n\nRStudio\nR Tools for Visual Studio\nRattle\nESS\nTinn-R\nR AnalyticalFlow\nRadiant\nRBox\nCode\n\n\n\n\nRStudio is one of the best integrated development environment (IDE) for R that includes a console, a terminal syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. RStudio Desktop and RStudio Server for Windows, Mac, and Linux are open sources.\nRStudio is now Posit. First, you have to download the latest version of RStudio according to your operating system from here.\n\nFor windows user, and just run the installation file and it normally detects your latest installed R version automatically. If you want to do some extra configuration, you need follow some steps which can be found here\nFor installation instruction of R and RStudio in Mac OS could be found here.\nInstalling R Studio on Ubuntu could be found here\nWe may open RStudio just double click Rstudio icon on your Desktop or on the task bar. The RStudio interface has four main panels:\n\nConsole: where you can type commands and see output.\nScript editor: where you can type out commands and save to file.\nEnvironment/History: environment shows all active objects and history keeps track of all commands run in console.\nFiles/Plots/Packages/Help etc.\n\n\n\nFigure 2: RStudio interface\n\n\n\n\n\n\nR Packages are collections of R functions, data, and compiled code in a well-defined format. The directory where packages are stored is called the library. We can install any R ‘package’ or multiple package directly from the console, using r-script and GUI (Tools > Install Packages) through internet.\nUse install.packages() function in your console or in a script:\n\ninstall.packages(“raster”, dependencies = TRUE)\n\nFor multiple packages:\n\ninstall.packages(c(“raster”,“gstat”), dependencies = TRUE)\n\nIf we want see content of any library, just use help() function or package specific:\n\nlibrary(help=spatial)\n\n\n\n\nR Markdown is an extension of the Markdown language that allows us to embed R code and its output within your documents. With R Markdown, we can create dynamic documents that integrate R code, results, and visualization with narrative text, all in a single document. R Markdown files have the extension “.Rmd” and can be rendered into various output formats such as HTML, PDF, Word, and more.\nR Markdown documents contain three main components: 1. The YAML header specifies the document’s metadata, such as the title, author, and output format.\n\nThe text body, contains the narrative text written in Markdown syntax.\nCode chunks are blocks of R code that can be executed and rendered within the document.\n\nR Markdown is a powerful tool for creating reproducible research documents, reports, presentations, and websites. It allows the integration of code, results, and visualization into a single document that others can easily share and reproduce.\nA brief overview could be found here:\nFor a brief tutorial, please visit here.\n\n\nTo create a new R Project, open RStudio and click on\nFile -> New Project\n\n\n\n\nTo create a new R Markdown document, open RStudio and click on\nFile -> New File -> R Markdown.\n\nYou will be prompted to choose a document format. Choose the format that suits your needs. In this tutorial, we will choose the “HTML” format.\nGive your document a title and author name, then click “Create”. This will create a new R Markdown document with the extension “.Rmd”.\n\nYour new R Markdown document will contain a template that you can use as a starting point. You can start adding content to your document by editing the template. The template contains a YAML header, which specifies the title, author, and output format of the document, and a Markdown body, which is where you can write your text.\n\nYou can add headings, lists, and links using Markdown syntax. You can also add inline R code and code chunks to your document. Once you have added content to your document, you can preview it by clicking on the “Knit” button at the top of the document editor. This will compile your R Markdown document into an output format (e.g. HTML, PDF, Word) based on the output format specified in the YAML header.\n\nHere below cheat sheet rmarkdown syntax:\n\n\nFigure 3: R-Markdown Cheat Sheet -1\n\n\n\n\n\nR-Markdown Cheat Sheet -1\n\n\n\n\n\nFigure 4: R-Markdown Cheat Sheet -2\n\n\n\n\n\n\n\nLike, R Markdown, Quarto is an open-source scientific and technical publishing system built on Pandoc, a universal document converter. It creates dynamic content with Python, R, Julia, and Observablefor publishing high-quality articles, reports, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nLike R Markdown, Quarto uses Knitr to execute R code. So it can render most existing Rmd files without modification. The difference between R Markdown and Quarto is related to output formats. Quarto includes many more built-in output formats (and more options for customizing each format). Quarto also has native features for particular project types like Websites, Books, and Blogs (rather than relying on external packages).\nTo run Quarto on RStudio, you must install it in your system. You can download it from here according to your operating system. After installation, you need to install the quarto package to render documents from the R console.\n\ninstall.packages(“quarto”)\n\n\nquarto::quarto_render(“hello.qmd”)\n\nAndy Field have series of YouTube video tutorials related to Quarto. Link of these are in below:\nQuarto part 1\nQuarto part 2\nQuarto part 3\nQuarto part 4\nQuarto part 5"
  },
  {
    "objectID": "getting-started-r.html",
    "href": "getting-started-r.html",
    "title": "Data Science with R For Envrionmental Modeling",
    "section": "",
    "text": "Getting Started with R\nRis a popular open-source programming language for statistical computing and graphics that can be used for DSM. It was developed in 1980 based on the S-language, and an open-source community regularly updates the software for a robust, programmable, portable, and open-source computing environment. We can use it to solve complex and sophisticated problems and “routine” analysis without restrictions on access or use.\nLearning R can be a challenging but rewarding experience.Here are some steps to get started with learning R:\nInstall R: You can download R for free from the official R website (https://www.r-project.org/). Choose the version of R that is appropriate for your operating system.\nInstall an IDE: An Integrated Development Environment (IDE) can help make coding in R easier. RStudio (https://rstudio.com/) is a popular and user-friendly IDE for R.\nLearn the basics: Start by learning the basic syntax and data types in R. You can find many online tutorials and resources to help you get started. The official R documentation (https://cran.r-project.org/manuals.html) is also a great resource.\nPractice: Practice coding in R by working on small projects and exercises. Kaggle (https://www.kaggle.com/) and DataCamp (https://www.datacamp.com/) offer many R courses and projects to help you improve your skills.\nJoin the R community: Joining the R community can help you learn from other R users and get answers to your questions. You can find R user groups in many cities, and there are also many online communities such as the RStudio Community (https://community.rstudio.com/).\nSome popular resources for learning Data Science with R include:\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nData Science in R by Roger D. Peng\nHands-On Machine Learning with R by Bradley Boehmke & Brandon Greenwell\nKaggle Learn\nGeographic Data Science with R\nSpatial Data Science with R and “terra”\nR for Geographic Data Science\nGeospatial Data Science With R"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science with R For Envrionmental Modeling",
    "section": "",
    "text": "Data science plays a critical role in environmental modeling by enabling scientists to collect, process, analyze, and interpret large amounts of data from various sources, including satellite imagery, remote sensors, field measurements, and other sources. Environmental modeling is creating computer-based models that simulate the behavior of the environment, such as the movement of air and water, the distribution of pollutants, and the effects of climate change. Data science can be used in environmental modeling in many ways. For example, the statistical predictive model can be used to predict future environmental conditions based on historical data, and data visualization techniques can be used to display complex environmental data in a way that is easy to understand. Additionally, data science can be used to identify patterns and trends in environmental data that might not be apparent through traditional data analysis techniques.\nAnother important area where data science is used in environmental modeling is spatial modeling since most environmental data are spatial. So spatial data science plays a critical role in environmental modeling by providing the tools and techniques needed to create highly accurate and detailed models of the environment. Spatial data science is a specialized area of data science that focuses on analyzing and interpreting spatial data, such as geographic information system (GIS) data, satellite imagery, and remote sensing data. In environmental modeling, spatial data science can be used to create highly detailed and accurate models of the environment, including the movement of air and water, the distribution of pollutants, and the effects of climate change. These models are used to forecast future environmental conditions, such as changes in temperature, rainfall, and sea levels. They can help researchers and policymakers make informed decisions about managing natural resources and mitigating climate change’s effects.\nData Science with R is a popular approach to analyzing data and creating data-driven models using the R programming language. R is an open-source programming language and environment that is widely used for statistical computing, data analysis, spatial data processing and analysis, and machine learning.\nData Science with R involves several steps, including data cleaning, exploratory data analysis, data visualization, statistical modeling, and machine learning. The R language offers a wide range of tools and libraries for each step, including dplyr and tidyr for data cleaning, drlook for data exploration, ggplot2 for data visualization, h20, caret and mlr for machine learning, and raster, rgdal, and gstat for spatial data processing and analysis.\nOverall, data science plays a crucial role in environmental modeling by enabling scientists to understand better the complex interactions between the environment and human activity and providing the tools and techniques to make accurate predictions about future environmental conditions.\nThis training-workshop will covered following lessons:\n\nGetting Started Digital Soil Mapping with R\nBasic R\n2.1. Download and Install R and R-Studio\n2.2. Introduction to use R\n2.3. Data Import-Export into/from R\nData Wrangling with R\n3.1.Introduction to Data Wragling\n3.2.Data Wrangling with dplyr and tidyr\n3.3.Data Wrangling with janitor\nIntroduction to Data Exploration and Visualization\n4.1.Introduction to Data Exploration and Visualization\n4.2. Basic Data Exploration and Visualization\n4.3.Data Exploration with dlookr\nRegression Analysis\n5.1.Introduction to Regression Analysis\n5.2 Simple Linear Regression\n5.3.Multiple Linear Regression\n5.4.Stepwise Regression\n5.5.Regression Model Evaluation\nMultivariate Statistic\n6.1 Introduction to Multivariate Statistics\n6.2.Principal Component Analysis (PCA)\n6.3.Factor Analysis\nMachine Learning\n7.1.Introduction to Machine Learning\n\n7.2. Regression Problem\n7.2.1. [Generalized Linear Models](generalized-linear-models.html)\n\n7.2.2. [Regularized Generalized Linear Models](regularized-glm.html)\n\n7.2.3. [Regression Trees](regression-trees.html)\n\n7.2.4. [Random Forest](random-forest.html)\n7.3. Classification Problem\n\nSpatial Data Processing\nDigital Terrain Modeling\nRemote Sensing\nSpatial Interpolation\n\nAfter finishing these lessons all participant will work on soil data and will produce digital soil maps of Bangladesh.\n\nZia U Ahmed, PhD\nResearch Associate Professor (Data & Visualization)\nRENEW (Research and Education in eNergy, Environment and Water) Institute\nUniversity at Buffalo\nWith the right skills and tools, Data Science with R can be a powerful approach to analyzing data and building predictive models for a wide range of applications."
  },
  {
    "objectID": "introduction-to-r.html",
    "href": "introduction-to-r.html",
    "title": "Data Science with R For Envrionmental Modeling",
    "section": "",
    "text": "R is an object-oriented programming language like Python, Julia, and JavaScript. Like these programming languages, R has a specific syntax or function, which is essential to understand if you want to use its features to accomplish thousands of things with R. However, one of the most challenging parts of learning R is finding your way around. In section of tutorial you will learn some basic of R such as syntax of R programming, assignment statements, r-data types, control statements and simple r-function.\n\n\nThe screen prompt > in R-console is an place to put command or instruction for R to work. Press the “Ctrl” + “L” keys simultaneously. The screen will now be refreshed and the console should be cleared.\n\n\nFigure 1: R Screen Prompt\n\n\n\n\n\n\nWe can use R as a calculators, at the prompt, we enter the expression that we want evaluated and when we hit enter, it will compute the result for us . For Example:\nFor addition:\n\n\nCode\n2+2\n\n\n[1] 4\n\n\nAnd for subtraction:\n\n\nCode\n4-2\n\n\n[1] 2\n\n\nFor multiplication:\n\n\nCode\n4*2\n\n\n[1] 8\n\n\nFor raised to the power:\n\n\nCode\n2^2\n\n\n[1] 4\n\n\nUse parentheses to ensure that it understands what you are trying to compute.\nhttps://www.geeksforgeeks.org/control-statements-in-r-programming/?ref=lbp\n\n\n\nVariables, Comments, and Keywords are the three main components in R- programming. Variables are used to store the data, Comments are used to improve code readability, and Keywords are reserved words that hold a specific meaning to the compiler.\n\n\n\nThere are so many built-in mathematical functions are available in base-R. Some are shown in below table:\n\n\nBuilt-in Math Functions\n\n\n\nHere below some examples of R built-in R-functions\n\n\nCode\nlog10(2)\n\n\n[1] 0.30103\n\n\n\n\nCode\nexp(1)\n\n\n[1] 2.718282\n\n\n\n\nCode\npi\n\n\n[1] 3.141593\n\n\n\n\nCode\nsin(pi/2)\n\n\n[1] 1\n\n\n\n\n\nWe can use very big numbers or very small numbers in R using the following scheme:\n\n\nCode\n1.2e3 # means 1200 because the e3 means ‘move the decimal point 3 places to the right \n\n\n[1] 1200\n\n\n\n\nCode\n1.2e-2 # means 0.012 because the e-2 means ‘move the decimal point 2 places to the left’\n\n\n[1] 0.012\n\n\n\n\n\nSuppose we want to know the integer part of a division: say, how many 13s are there in 119:\n\n\nCode\n119 %/% 13\n\n\n[1] 9\n\n\nSuppose we wanted to know the remainder (what is left over when 119 is divided by 13: in maths this is known as modulo\n\n\nCode\n119 %% 13\n\n\n[1] 2\n\n\n\n\n\nSeveral types of rounding (rounding up, rounding down, rounding to the nearest integer) can be done easily with R.\nThe ‘greatest integer less than’ function is floor()\n\n\nCode\nfloor(5.7)\n\n\n[1] 5\n\n\nThe ‘next integer’ function is ceiling()\n\n\nCode\nceiling(5.7)\n\n\n[1] 6\n\n\n\n\n\nJust like in algebra, we often want to store a computation under some variable name. The result is assigned to a variable with the symbols = or <- which is formed by the “less than” symbol followed immediately by a hyphen.\n\n\nCode\nx<-10; # or\ny = 12\n\n\nWhen you want to know what is in a variable simply ask by typing the variable name.\n\n\nCode\nx; # or\n\n\n[1] 10\n\n\nCode\ny\n\n\n[1] 12\n\n\nWe can store a computation of two variable names and do some calculation and the result is assigned to a new variable\n\n\nCode\na=2;\nb=3;\nc=a+b;\nc\n\n\n[1] 5\n\n\n\n\n\n\nDo not begin a variable name with a period or a number. Variable names are case (upper/lower) sensitive.\nVariable names in R are case-sensitive so x is not the same as X.\nVariable names should not begin with numbers (e.g. 1x) or symbols (e.g. %x).\nVariable names should not contain blank spaces: use grain.yield\n\n\n\n\n\n\nCode\n# + - */%% ^ arithmetic\n# > >= < <= == != relational\n# ! & \u0003 logical\n# ~ model formulae\n# <- -> assignment\n# $ list indexing (the ‘element name’ operator)\n# : create a sequence\n\n\n\n\n\nR has a wide variety of data types including scalars, vectors (numerical, character, logical), matrices, data frames, and lists.You can check the type of a variable using the class() function. For example:\n\n\nCode\nx <- 5\nclass(x) # numeric\n\n\n[1] \"numeric\"\n\n\nCode\ny <- \"hello\"\nclass(y) # character\n\n\n[1] \"character\"\n\n\nCode\nz <- TRUE\nclass(z) # logical\n\n\n[1] \"logical\"\n\n\n\n\nA vector is a basic data structure in R that can hold multiple values of the same data type.\nA scalar data structure is the most basic data type that holds only a single atomic value at a time. Using scalars, more complex data types can be constructed. The most commonly used scalar types in R:\n\nNumeric\nCharacter or strings\nInteger\nLogical\nComplex\n\nNumeric is the default type used in R for mathematical computations. Examples of numeric are decimal numbers and whole numbers.\n\n\nCode\nx=1.2\nx\n\n\n[1] 1.2\n\n\nCode\nclass(x)\n\n\n[1] \"numeric\"\n\n\nCharacter objects are strings. They could be any sequence of characters including alphabets, numbers, punctuation marks, etc. enclosed in quotes.\n\n\nCode\nDepartment = 'Chemistry'\nSchool= \"University at Buffalo\"\nclass(School)\n\n\n[1] \"character\"\n\n\nCode\npaste(Department,\",\", School)\n\n\n[1] \"Chemistry , University at Buffalo\"\n\n\nLogical values are boolean values of TRUE or FALSE. Note that R needs logical values of TRUE or FALSE to be in upper case. If you use mixed case or lowercase, you’ll get an error or unpredictable results.\n\n\nCode\nu = TRUE; \nv = FALSE\nclass(u)\n\n\n[1] \"logical\"\n\n\nCode\nclass(v)\n\n\n[1] \"logical\"\n\n\nA list of numbers or charterers together to form a Multiple Elements Vector. Values can be assigned to vectors in many different ways. We can create a vector of number from 1 to 10, using the concatenation function c\n\n\nCode\na <- c(1,2,5.3,6,7,8,9,10)\na\n\n\n[1]  1.0  2.0  5.3  6.0  7.0  8.0  9.0 10.0\n\n\n\n\nCode\ns <- c('apple','red',5,TRUE)\nprint(s)\n\n\n[1] \"apple\" \"red\"   \"5\"     \"TRUE\" \n\n\nIt can be generated by the sequence of integer values 1 to 10 using : (colon), the sequence-generating operator,\n\n\nCode\na<-1:10\na\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can also create a vector using Using sequence (Seq.) operator.\n\n\nCode\n# Create vector with elements from 5 to 9 incrementing by 0.4.\nb = seq(5, 9, by = 0.4)\nb\n\n\n [1] 5.0 5.4 5.8 6.2 6.6 7.0 7.4 7.8 8.2 8.6 9.0\n\n\nR has ability to evaluate functions over entire vectors, so no need to write , for loops and subscripts. Important vector functions are listed in below Table:\n\n\nVector Functions\n\n\n\nOnce we have a vector of numbers we can apply certain built-in functions to them to get useful summaries. For example:\n\n\nCode\nsum(a)        # sums the values in the vector \n\n\n[1] 55\n\n\nCode\nlength(a)     # number of the values in the vector \n\n\n[1] 10\n\n\nCode\nmean (a)      # the average of the values in the vector \n\n\n[1] 5.5\n\n\nCode\nvar (a)        # the sample variance of the values \n\n\n[1] 9.166667\n\n\nCode\nsd(a)         # the standard of deviations of the values  \n\n\n[1] 3.02765\n\n\nCode\nmax(a)        # the largest value in the vector  \n\n\n[1] 10\n\n\nCode\nmin(a)        # the smallest number in the vector \n\n\n[1] 1\n\n\nCode\nmedian(a)     # the sample median \n\n\n[1] 5.5\n\n\nSummary() function will calculate summary statistics of a vector\n\n\nCode\nsummary(a)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00 \n\n\nTwo vectors of same length can be added, subtracted, multiplied or divided giving the result as a vector output.\n\n\nCode\n# Create two vectors.\nv1 <- c(3,8,4,5,0,11)\nv2 <- c(4,11,0,8,1,2)\n\n\n\n\nCode\n# Vector addition.\nadd.result <- v1+v2\nprint(add.result)\n\n\n[1]  7 19  4 13  1 13\n\n\n\n\nCode\n# Vector subtraction.\nsub.result <- v1-v2\nprint(sub.result)\n\n\n[1] -1 -3  4 -3 -1  9\n\n\n\n\nCode\n# Vector multiplication.\nmulti.result <- v1*v2\nprint(multi.result)\n\n\n[1] 12 88  0 40  0 22\n\n\n\n\nCode\n# Vector division.\ndivi.result <- v1/v2\nprint(divi.result)\n\n\n[1] 0.7500000 0.7272727       Inf 0.6250000 0.0000000 5.5000000\n\n\n\n\n\nMatrices is a two-dimensional rectangular layout of number in rows and columns. All columns in a matrix must have the same mode (numeric, character, etc.) and the same length.\nAll columns in a matrix must have the same mode (numeric, character, etc.) and the same length. There are several ways of making a matrix. Suppose you were interested in the matrix of 2 x 3. You could form the two rows (vectors) and then bind (rbind) them together to form the matrix:\n\n\nCode\nr1=c(6,2,10)     # row 1\nr2=c(1,3,-2)     # row 2\nX=rbind(r1,r2)   # binds the vectors into rows a matrix\nX\n\n\n   [,1] [,2] [,3]\nr1    6    2   10\nr2    1    3   -2\n\n\nCode\nclass(X)\n\n\n[1] \"matrix\" \"array\" \n\n\nWe can bind them (cbind) the same vectors into columns of a matrix\n\n\nCode\nY=cbind(r1,r2)   \nY\n\n\n     r1 r2\n[1,]  6  1\n[2,]  2  3\n[3,] 10 -2\n\n\nA Matrix cab be created using the matrix() function from the given set of values. The basic function of a matrix is:\n\nmatrix(data, nrow, ncol, byrow, dimnames)\n\nThe values are:\n\ndata is the input vector which becomes the data elements of the matrix.\nnrow is the number of rows to be created.\nncol is the number of columns to be created.\nbyrow is a logical clue. If TRUE then the input vector elements are arranged by row.\ndimname is the names assigned to the rows and columns.\n\n\n\nCode\nX <- matrix(1:9, nrow = 4, ncol = 3, byrow=T) # row matrix\n\n\nWarning in matrix(1:9, nrow = 4, ncol = 3, byrow = T): data length [9] is not a\nsub-multiple or multiple of the number of rows [4]\n\n\nCode\nX\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]    1    2    3\n\n\n\n\nCode\nclass(X)\n\n\n[1] \"matrix\" \"array\" \n\n\nCode\nattributes(X)\n\n\n$dim\n[1] 4 3\n\n\nThe class and attributes of X indicate that it is a matrix of four rows and three columns (these are its dim attributes)\nWe can create matrix with row and column names:\n\n\nCode\n# create a vector \ncells=c(1,26,24,68,35,68,73,18,2,56,4,5,34,21,24,20)  # create a vector\n# names of column rows\ncnames = c(\"C1\",\"C2\",\"C3\",\"C4\") \n# names of two rows\nrnames = c(\"R1\",\"R2\",\"R3\",\"R4\") \n# matrix\nZ= matrix(cells,nrow=4, ncol=4, byrow=TRUE,dimnames=list(rnames,cnames))\nZ\n\n\n   C1 C2 C3 C4\nR1  1 26 24 68\nR2 35 68 73 18\nR3  2 56  4  5\nR4 34 21 24 20\n\n\nOr, we can easily naming the rows and columns of matrices. Suppose we want to labels rows with Trial names, like Trial.1, Trial.2 etc.:\n\n\nCode\nrownames(X)<-rownames(X, do.NULL=FALSE, prefix=\"Trial.\")\nX\n\n\n        [,1] [,2] [,3]\nTrial.1    1    2    3\nTrial.2    4    5    6\nTrial.3    7    8    9\nTrial.4    1    2    3\n\n\nFor column names, we will create a vector of different names for the three most commonly used drugs used in the trial, and use this to specify the colnames(X):\n\n\nCode\ndrug.names<-c(\"Aspirin\", \"Acetaminophen\", \"Ibuprofen\")\ncolnames(X)<-drug.names\nX\n\n\n        Aspirin Acetaminophen Ibuprofen\nTrial.1       1             2         3\nTrial.2       4             5         6\nTrial.3       7             8         9\nTrial.4       1             2         3\n\n\nWe can access elements of a matrix using the square bracket [] indexing method. Elements can be accessed as var[row, column]. Here rows and columns are vectors.\n\n\nCode\nX[,2]  # 2nd column of a matrix\n\n\nTrial.1 Trial.2 Trial.3 Trial.4 \n      2       5       8       2 \n\n\nCode\nX[3,]  # 3rd row of a matrix\n\n\n      Aspirin Acetaminophen     Ibuprofen \n            7             8             9 \n\n\n\n\nCode\nX[,2:3] # 2nd and 3rd column\n\n\n        Acetaminophen Ibuprofen\nTrial.1             2         3\nTrial.2             5         6\nTrial.3             8         9\nTrial.4             2         3\n\n\n\n\nCode\nX[2:4,1:2]     # rows 2,3,4 of columns 1 and 2\n\n\n        Aspirin Acetaminophen\nTrial.2       4             5\nTrial.3       7             8\nTrial.4       1             2\n\n\nWe can use summary() function to get row and column wise summary statistics of a matrix\n\n\nCode\n# summary statistics of each column\nsummary(X)\n\n\n    Aspirin     Acetaminophen    Ibuprofen   \n Min.   :1.00   Min.   :2.00   Min.   :3.00  \n 1st Qu.:1.00   1st Qu.:2.00   1st Qu.:3.00  \n Median :2.50   Median :3.50   Median :4.50  \n Mean   :3.25   Mean   :4.25   Mean   :5.25  \n 3rd Qu.:4.75   3rd Qu.:5.75   3rd Qu.:6.75  \n Max.   :7.00   Max.   :8.00   Max.   :9.00  \n\n\n\n\nCode\n# summary statistics and mean of the column 1 of matrix\nsummary(X[,1])\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    2.50    3.25    4.75    7.00 \n\n\nCode\n# mean\nmean(X[,1])\n\n\n[1] 3.25\n\n\nCalculated over all the rows and the mean & variance of the bottom row (Trial.4)\n\n\nCode\nmean(X[4,])\n\n\n[1] 2\n\n\nCode\nvar(X[4,])\n\n\n[1] 1\n\n\nThere are some special functions for calculating summary statistics on matrices\n\n\nCode\n# Total\nrowSums(X)\n\n\nTrial.1 Trial.2 Trial.3 Trial.4 \n      6      15      24       6 \n\n\nCode\ncolSums(X)\n\n\n      Aspirin Acetaminophen     Ibuprofen \n           13            17            21 \n\n\n\n\nCode\n# Mean\nrowMeans(X)\n\n\nTrial.1 Trial.2 Trial.3 Trial.4 \n      2       5       8       2 \n\n\nCode\ncolMeans(X)\n\n\n      Aspirin Acetaminophen     Ibuprofen \n         3.25          4.25          5.25 \n\n\nWe can also use apply() function to calculate row and column means. Here columns are margin no. 2 (rows are margin no. 1\n\n\nCode\napply(X,2,mean)\n\n\n      Aspirin Acetaminophen     Ibuprofen \n         3.25          4.25          5.25 \n\n\n\n\nCode\napply(X,1,mean)\n\n\nTrial.1 Trial.2 Trial.3 Trial.4 \n      2       5       8       2 \n\n\n\n\n\nFactors are data structures that are implemented to categorize the data or represent categorical data and store it on multiple levels.\nIn R, factor() function create or convert string-vectors to factors:\n\n\nCode\n# string vectors\ngender <- c(rep(\"male\",20), rep(\"female\", 30))\n# define factors\ngender <- factor(gender) # # 1=female, 2=male internally (alphabetically)\n# checking the factors\nprint(is.factor(gender))\n\n\n[1] TRUE\n\n\nCode\nclass(gender) # \n\n\n[1] \"factor\"\n\n\nCode\nsummary(gender)\n\n\nfemale   male \n    30     20 \n\n\n\n\n\nList is a one-detrimental data element which consist of several objects in a order. The object in a list may be mixed data types or different data types.The list can be a list of vectors, a list of matrices, a list of characters and a list of functions, and so on.\nlist in R is created with the use of list() function.\n\n\nCode\nmy.list <- list(Location=\"NY\", \n                Year = 2021,\n                LabExp=X) # Lab experimental data\n             \n\nlist(my.list)\n\n\n[[1]]\n[[1]]$Location\n[1] \"NY\"\n\n[[1]]$Year\n[1] 2021\n\n[[1]]$LabExp\n        Aspirin Acetaminophen Ibuprofen\nTrial.1       1             2         3\nTrial.2       4             5         6\nTrial.3       7             8         9\nTrial.4       1             2         3\n\n\nComponents of a list can be accessed in similar fashion like matrix or data frame:\n\n\nCode\nmy.list[\"LabExp\"]\n\n\n$LabExp\n        Aspirin Acetaminophen Ibuprofen\nTrial.1       1             2         3\nTrial.2       4             5         6\nTrial.3       7             8         9\nTrial.4       1             2         3\n\n\nCode\nmy.list[\"FieldData\"]\n\n\n$<NA>\nNULL\n\n\n\n\n\nIn R, tabular data are stored as Data Frame which is made up of three principal components, the data, rows, and columns. It is more general than a matrix, in that different columns can have different modes (numeric, character, factor, etc.).\nTo create a data frame in R use data.frame() command and then pass each of the vectors you have created as arguments to the function\n\n\nCode\nID = c(1,2,3,4)    # create a vector of ID coloumn \nLandcover = c(\"Grassland\",\"Forest\", \"Arable\", \"Urban\") # create a text vector \nSettlement  = c (FALSE, FALSE, FALSE, TRUE) # creates a logical vector\npH   = c(6.6,4.5, 6.8, 7.5)   # create a numerical vector\nSOC  = c (1.2, 3.4, 1.1, 0.12) # create a numerical vector\nmy.df=data.frame(ID,Landcover,Settlement, pH, SOC) # create a data frame\n\nmy.df\n\n\n  ID Landcover Settlement  pH  SOC\n1  1 Grassland      FALSE 6.6 1.20\n2  2    Forest      FALSE 4.5 3.40\n3  3    Arable      FALSE 6.8 1.10\n4  4     Urban       TRUE 7.5 0.12\n\n\nwe can see the detail of structure using str() function\n\n\nCode\nstr(my.df)\n\n\n'data.frame':   4 obs. of  5 variables:\n $ ID        : num  1 2 3 4\n $ Landcover : chr  \"Grassland\" \"Forest\" \"Arable\" \"Urban\"\n $ Settlement: logi  FALSE FALSE FALSE TRUE\n $ pH        : num  6.6 4.5 6.8 7.5\n $ SOC       : num  1.2 3.4 1.1 0.12\n\n\n\n\nCode\nhead(my.df)\n\n\n  ID Landcover Settlement  pH  SOC\n1  1 Grassland      FALSE 6.6 1.20\n2  2    Forest      FALSE 4.5 3.40\n3  3    Arable      FALSE 6.8 1.10\n4  4     Urban       TRUE 7.5 0.12\n\n\n\n\nCode\nsummary(my.df$pH)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  4.500   6.075   6.700   6.350   6.975   7.500 \n\n\nCode\nsummary(my.df[,4:5])\n\n\n       pH             SOC       \n Min.   :4.500   Min.   :0.120  \n 1st Qu.:6.075   1st Qu.:0.855  \n Median :6.700   Median :1.150  \n Mean   :6.350   Mean   :1.455  \n 3rd Qu.:6.975   3rd Qu.:1.750  \n Max.   :7.500   Max.   :3.400  \n\n\nComponents of data frame can be accessed like a list or like a matrix.\n\n\nCode\nmy.df[\"Landcover\"]\n\n\n  Landcover\n1 Grassland\n2    Forest\n3    Arable\n4     Urban\n\n\nCode\nmy.df[[2]]\n\n\n[1] \"Grassland\" \"Forest\"    \"Arable\"    \"Urban\"    \n\n\nCode\nmy.df[,4:5]\n\n\n   pH  SOC\n1 6.6 1.20\n2 4.5 3.40\n3 6.8 1.10\n4 7.5 0.12\n\n\n\n\n\n\nControl statements are programming language constructs that allow the programmer to control the flow of execution of a program. They are used to alter the order in which statements are executed, and to make decisions based on conditions.\nThe eight major types of control statements are follows:\n\nif: statement for conditional programming\nif..else: statement for conditional programming\nfor: loop to iterate over a fixed number of iterations\nwhile: loop to iterate until a logical statement returns FALSE\nrepeat: loop to execute until told to break\nbreak/next:break/next arguments to exit and skip interations in a loop\n\n\n\nIf the expression is true, the statement gets executed. But if the expression is FALSE, nothing happens.\n\n\nCode\nx <- 12\n# condition\nif(x > 10){\nprint(paste(x, \"is greater than 10\"))\n}\n\n\n[1] \"12 is greater than 10\"\n\n\n\n\n\nIt is similar to if condition but when the test expression in if condition fails, then statements in else condition are executed.\n\n\nCode\nx <- c(3, 3, -2, 1)\n\nif(any(x < 0)){\n        print(\"x contains negative numbers\")\n} else{\n        print(\"x contains all positive numbers\")\n}\n\n\n[1] \"x contains negative numbers\"\n\n\n\n\nCode\nx <- c(3, 3, 3, 1)\n\nif(any(x < 0)){\n        print(\"x contains negative numbers\")\n} else{\n        print(\"x contains all positive numbers\")\n}\n\n\n[1] \"x contains all positive numbers\"\n\n\n\n\n\nThe for loop is used to execute repetitive code statements for a particular number of time. It is useful to iterate over the elements of a list, dataframe, vector, matrix, or any other object.\n\n\nCode\nfor (i in 10:15){\n        output <- paste(\"The number is\", i)\n        print(output)\n}\n\n\n[1] \"The number is 10\"\n[1] \"The number is 11\"\n[1] \"The number is 12\"\n[1] \"The number is 13\"\n[1] \"The number is 14\"\n[1] \"The number is 15\"\n\n\n\n\nCode\n# for loop with vector\nx <- c(-8, 3, 12, 15)\nfor (i in x)\n{\n    print(i)\n}\n\n\n[1] -8\n[1] 3\n[1] 12\n[1] 15\n\n\n\n\n\nWhile loop executes the same code again and again until a stop condition is met\n\n\nCode\nresult <- 1\ni <- 1\n# test expression\nwhile (i < 5) {\n    print(result)\n# update expression\n   i = i + 1\n   result = result + 1\n}\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n\n\nFollowing example show the while statement with break\n\n\nCode\nresult <- 1\ni <- 1\n# test expression\nwhile (i < 5) {\n    print(result)\n# add break after 2 element \n  if (i==2){\n    break\n  }\n# update expression\n   i = i + 1\n   result = result + 1\n}\n\n\n[1] 1\n[1] 2\n\n\n\n\n\n\nWriting custom functions is an important part of programming in R.\nTo create a new R function we need to think about 4 major things:\n\nthe name of the function\nthe arguments (inputs) the function will take\nthe code the function will run\nthe output the function will return for the user\n\nTo create a function, use the function() keyword:\n\n\nCode\n# create a function with the name my_function\nmy_function <- function() { \n  print(\"Hello World!\")\n}\n# call the function\nmy_function()\n\n\n[1] \"Hello World!\"\n\n\nArguments are specified after the function name, inside the parentheses. The following example has a function (full_name) with one arguments (last_name). When the function is called, we pass along a first name, which is used inside the function to print the full name:\n\n\nCode\nfulll_name <- function(last_name) {\n  paste(\"Zia\",  last_name)\n}\nfulll_name(\"Ahmed\")\n\n\n[1] \"Zia Ahmed\"\n\n\nTo return the results of a function, use the return() function:\n\n\nCode\naddition <- function(x) {\n  return (1 + x)\n}\nprint(addition(2))\n\n\n[1] 3\n\n\nCode\nprint(addition(3))\n\n\n[1] 4\n\n\nWe can create a simple equation with two arguments (x, y):\n\n\nCode\nequation <- function(x, y) {\n  a <- x + y\n  return(a)\n}\nequation(2,2)\n\n\n[1] 4\n\n\nWe can Call a function within another function:\n\n\nCode\nequation(equation(2,4), equation(3,3))\n\n\n[1] 12\n\n\n\nThe output above function is therefore (2+4) + (3+3) = 12.\n\nWe can also write a function within a function:\n\n\nCode\nx <- 10 \ny<- function() {\n        r <- 2\n        n <- 5\n        z <- function() {\n                (1+r)^n\n        }\n        x/z()\n}\ny()\n\n\n[1] 0.04115226\n\n\nReturning Multiple Outputs from a Function:\n\n\nCode\nresults_all <- function(x, y) {\n        results1 <- 2*x + y\n        results2 <- x + 2*y\n        results3 <- 2*x + 2*y\n        results4 <- x/y\n        c(results1, results2, results3, results4)\n}\nresults_all(1, 2)\n\n\n[1] 4.0 5.0 6.0 0.5\n\n\nFollowing function shows an example to convert temperature from Celsius (C) to Fahrenheit (F):\n\n\nCode\nC_to_F = function(C) {\n f = (9/5) * C + 32; # formula\n return(f); # return to\n}\nC_to_F(10)\n\n\n[1] 50\n\n\nCode\nC= c(4:10)\nC_to_F(C)\n\n\n[1] 39.2 41.0 42.8 44.6 46.4 48.2 50.0\n\n\n\n\n\nThe apply family consists of vectorized functions which minimize our need to explicitly create loops. These family is an inbuilt R package, so no need to install any packages for the execution.\n\napply() for matrices and data frames\nlapply() for lists…output as list\nsapply() for lists…output simplified\ntapply() for vectors\nmapply() for multi-variant\n\n\n\napply() returns a vector or array or list of values obtained by applying a function to margins of an array or matrix or dataframe. Using apply() is not faster than using a loop function, but it is highly compact and can be written in one line.\n\napply(x,MARGIN, FUN,…)\n\nWhere:\n\nx is the matrix, dataframe or array\nMARGIN is a vector giving the subscripts which the function will be applied over. E.g., for a matrix 1 indicates rows, 2 indicates columns, c(1, 2) indicates rows and columns.\nFUN is the function to be applied\n… is for any other arguments to be passed to the function\n\n\n\nCode\n# Crate a dataframe\ndf <- cbind(x1 = 1:8, x2 = 2:9, x3=3:10)\n# add row names\ndimnames(df)[[1]] <- letters[1:8] \n\n\nLet’s calculate column mean:\n\n\nCode\napply(df, 2, mean, trim = 0.2)\n\n\n x1  x2  x3 \n4.5 5.5 6.5 \n\n\nRow mean:\n\n\nCode\napply(df, 1, mean, trim = .2)\n\n\na b c d e f g h \n2 3 4 5 6 7 8 9 \n\n\nGet column quantile:\n\n\nCode\napply(df, 2, quantile, probs = c(0.10, 0.25, 0.50, 0.75, 0.90))\n\n\n      x1   x2   x3\n10% 1.70 2.70 3.70\n25% 2.75 3.75 4.75\n50% 4.50 5.50 6.50\n75% 6.25 7.25 8.25\n90% 7.30 8.30 9.30\n\n\n\n\n\nlapply() returns a list of the same length as X (list), each element of which is the result of applying FUN to the corresponding element of X. It loops over a list, iterating over each element in that list and then applies a function to each element of the list and finally returns a list (l stand for list).\n\nlapply(x, FUN, …)\n\nWhere:\n\nx is the list\nFUN is the function to be applied\n… is for any other arguments to be passed to the function\n\n\n\nCode\n# Create a list\nmylist<-list(A=matrix(1:9,nrow=3),B=1:5,C=c(8,5),  logic = c(TRUE,FALSE,FALSE,TRUE, TRUE))\nmylist\n\n\n$A\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n$B\n[1] 1 2 3 4 5\n\n$C\n[1] 8 5\n\n$logic\n[1]  TRUE FALSE FALSE  TRUE  TRUE\n\n\n\n\nCode\nlapply(mylist, mean)\n\n\n$A\n[1] 5\n\n$B\n[1] 3\n\n$C\n[1] 6.5\n\n$logic\n[1] 0.6\n\n\nYou can see how the results are saved as a list form. We can easily unlist the results:\n\n\nCode\nunlist(lapply(mylist,mean))\n\n\n    A     B     C logic \n  5.0   3.0   6.5   0.6 \n\n\n\n\n\nsapply() is a wrapper of lapply() to simplify the result to vector or matrix.\n\n\nCode\nsapply(mylist, mean)\n\n\n    A     B     C logic \n  5.0   3.0   6.5   0.6 \n\n\n\n\n\ntapply() is used to apply a function over subsets of a vector when a dataset can be broken up into groups (via categorical variables - aka factors)\n\n\nCode\nmy.df\n\n\n  ID Landcover Settlement  pH  SOC\n1  1 Grassland      FALSE 6.6 1.20\n2  2    Forest      FALSE 4.5 3.40\n3  3    Arable      FALSE 6.8 1.10\n4  4     Urban       TRUE 7.5 0.12\n\n\nWe can use tapply() to calculate mean values of pH an SOC for land cover\n\n\nCode\napply(my.df[4:5], 2, function(x) tapply(x, my.df$Landcover, mean))\n\n\n           pH  SOC\nArable    6.8 1.10\nForest    4.5 3.40\nGrassland 6.6 1.20\nUrban     7.5 0.12\n\n\n\n\n\nmapply() is a multivariate version of sapply(). mapply() applies FUN to the first elements of each … argument, the second elements, the third elements, and so on.\n\n\nCode\nlist( rep(2, 4), rep(3, 3), rep(4, 2))\n\n\n[[1]]\n[1] 2 2 2 2\n\n[[2]]\n[1] 3 3 3\n\n[[3]]\n[1] 4 4\n\n\nYou can see that the same function (rep) is being called repeatedly where the first argument (number vector) varies from 2 to 4, and the second argument (rep) varies from 4 to 2. Instead, you can use mapply()\n\n\nCode\nmapply(rep, 2:4, 4:2)\n\n\n[[1]]\n[1] 2 2 2 2\n\n[[2]]\n[1] 3 3 3\n\n[[3]]\n[1] 4 4\n\n\n\n\n\n\n\nAn Introduction to R - The Comprehensive R Archive\nR Introduction - W3Schools\nAn Introduction to R"
  }
]